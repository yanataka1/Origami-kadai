{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1ab5693",
   "metadata": {},
   "source": [
    "# 線形回帰\n",
    "\n",
    "線形回帰のクラスをスクラッチで作成していきます。NumPyなど最低限のライブラリのみを使いアルゴリズムを実装していきます。\n",
    "\n",
    "\n",
    "以下に雛形を用意してあります。このScratchLinearRegressionクラスにコードを書き加えていってください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9306d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from numpy.random import *\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import mean_squared_error\n",
    "sns.set()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e31b4504",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "完成例：雛形\n",
    "\n",
    "スクラッチコードの完成形\n",
    "線形回帰のクラスをスクラッチで作成していきますが、最終的なコードはどのようになっているのでしょうか。\n",
    "\n",
    "下記は、最終的なコードの概観になります。\n",
    "\"\"\"\n",
    "class ScratchLinearRegression():\n",
    "    def __init__(self,・・・):\n",
    "      \"\"\"\n",
    "      インスタンス変数初期化\n",
    "      \"\"\"\n",
    "      ・・・\n",
    "\n",
    "    # 問題6（学習と推定）\n",
    "    def fit(self,・・・):\n",
    "        \"\"\"\n",
    "        線形回帰の学習\n",
    "        \"\"\"\n",
    "        # メイン処理\n",
    "        for i in range(学習回数):\n",
    "            # 問題1（過程関数の計算）\n",
    "            self._linear_hypothesis(・・・)\n",
    "\n",
    "            # 問題2（最急降下法によるパラメータの更新値計算）\n",
    "            self._gradient_descent(・・・)\n",
    "\n",
    "            # 問題7（学習曲線のプロット）のグラフ描画時（問題5（損失関数）で作成した関数を使用）\n",
    "            self._loss_func(・・・)\n",
    "\n",
    "\n",
    "    # 問題1\n",
    "    def _linear_hypothesis(self,・・・):\n",
    "        \"\"\"\n",
    "        仮定関数の計算\n",
    "        \"\"\"\n",
    "\n",
    "    # 問題2\n",
    "    def _gradient_descent(self,・・・):\n",
    "        \"\"\"\n",
    "        最急降下法によるパラメータの更新値計算\n",
    "        \"\"\"\n",
    "\n",
    "    # 問題3\n",
    "    def predict(self,・・・):\n",
    "        \"\"\"\n",
    "        線形回帰での推定\n",
    "        \"\"\"\n",
    "\n",
    "    # 問題4\n",
    "    def _mse(self,・・・):\n",
    "        \"\"\"\n",
    "        平均二乗誤差の計算\n",
    "        \"\"\"\n",
    "\n",
    "    # 問題5\n",
    "    def _loss_func(self,・・・):\n",
    "        \"\"\"\n",
    "        損失関数\n",
    "        \"\"\"\n",
    "        # 問題4\n",
    "        self._mse(・・・)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4e9b66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchLinearRegression():\n",
    "    \"\"\"\n",
    "    線形回帰のスクラッチ実装\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    num_iter : int\n",
    "      イテレーション数\n",
    "    lr : float\n",
    "      学習率\n",
    "    no_bias : bool\n",
    "      バイアス項を入れない場合はTrue\n",
    "    verbose : bool\n",
    "      学習過程を出力する場合はTrue\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    self.coef_ : 次の形のndarray, shape (n_features,)\n",
    "      パラメータ\n",
    "    self.loss : 次の形のndarray, shape (self.iter,)\n",
    "      訓練データに対する損失の記録\n",
    "    self.val_loss : 次の形のndarray, shape (self.iter,)\n",
    "      検証データに対する損失の記録\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_iter, lr, no_bias, verbose):\n",
    "        # ハイパーパラメータを属性として記録\n",
    "        #self.theta = np.ones((theta, 1))\n",
    "        \n",
    "        self.iter = num_iter\n",
    "        self.lr = lr\n",
    "        self.no_bias = no_bias\n",
    "        self.verbose = verbose\n",
    "        # 損失を記録する配列を用意\n",
    "        self.loss = np.zeros(self.iter)\n",
    "        self.val_loss = np.zeros(self.iter)\n",
    "        \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        線形回帰を学習する。検証データが入力された場合はそれに対する損失と精度もイテレーションごとに計算する。\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            訓練データの正解値\n",
    "        X_val : 次の形のndarray, shape (n_samples, n_features)\n",
    "            検証データの特徴量\n",
    "        y_val : 次の形のndarray, shape (n_samples, )\n",
    "            検証データの正解値\n",
    "        \"\"\"\n",
    "        #問題１　仮定関数の計算\n",
    "        self.y_hat = self._linear_hypothesis(X)\n",
    "        \n",
    "        #問題２　最急降下法によるパラメータの更新値計算\n",
    "        \n",
    "        \n",
    "        self.theta = self._gradient_descent(X, error)\n",
    "        \n",
    "        if self.verbose:\n",
    "            #verboseをTrueにした際は学習過程を出力\n",
    "            print()\n",
    "        pass\n",
    "    # 問題１　仮定関数の計算\n",
    "    def _linear_hypothesis(self,X):\n",
    "        #self.theta = X.shape[1] いらない\n",
    "        #print(self.theta)\n",
    "        #print(X)\n",
    "        \n",
    "        self.theta = np.ones((self.theta, 1)) #init?\n",
    "        self.y_hat = X @ self.theta\n",
    "        \n",
    "        return self.y_hat\n",
    "    \n",
    "    #　問題２　最急降下法によるパラメータの更新値計算\n",
    "    def _gradient_descent(self, X, error, theta, alpha):\n",
    "        \n",
    "        error = self.y_hat - y\n",
    "        alpha = 0.01\n",
    "        #下はメンターコード\n",
    "        #self.Theta = self.Theta - 0.01 * ((self.error.T * X).sum()) / x.shape[0]\n",
    "        self.theta = self.theta - alpha * (error.T @ X) / X.shape[0]\n",
    "    \n",
    "        return self.theta\n",
    "    \n",
    "    #問題３　線形回帰での推定\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        線形回帰を使い推定する。\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            サンプル\n",
    "        Returns\n",
    "        -------\n",
    "            次の形のndarray, shape (n_samples, 1)\n",
    "            線形回帰による推定結果\n",
    "        \"\"\"\n",
    "        \n",
    "        self.y_hat = self._linear_hypothesis(X)\n",
    "        \n",
    "        return y_hat\n",
    "    \n",
    "    # 問題４　平均二乗誤差の計算\n",
    "    def _mse(y, y_hat):\n",
    "        \"\"\"\n",
    "        平均二乗誤差の計算\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_pred : 次の形のndarray, shape (n_samples,)\n",
    "          推定した値\n",
    "        y : 次の形のndarray, shape (n_samples,)\n",
    "          正解値\n",
    "        Returns\n",
    "        ----------\n",
    "        mse : numpy.float\n",
    "              平均二乗誤差\n",
    "        \"\"\"\n",
    "        mse = ((self.y_hat-y)**2).sum()/ (y.shape[0])\n",
    "    \n",
    "        return mse\n",
    "    \n",
    "    # 問題5 損失関数\n",
    "    def _loss_func(self, y, y_hat):\n",
    "        \n",
    "        loss = self._mse(y, y_hat) / 2\n",
    "        return loss \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c68d6f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(vars(self))でクラスの中身がわかる"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db488eb",
   "metadata": {},
   "source": [
    "## 【問題1】仮定関数\n",
    "以下の数式で表される線形回帰の仮定関数を実装してください。メソッドの雛形を用意してあります。\n",
    "\n",
    "$h_θ(x)=θ_0x_0+θ_1x_1+...+θ_jx_j+...+θ_nx_n.(x_0=1)$\n",
    "\n",
    "x\n",
    " : 特徴量ベクトル\n",
    "\n",
    "\n",
    "θ\n",
    " : パラメータベクトル\n",
    "\n",
    "\n",
    "n\n",
    " : 特徴量の数\n",
    "\n",
    "\n",
    "x\n",
    "j\n",
    " : j番目の特徴量\n",
    "\n",
    "\n",
    "θ\n",
    "j\n",
    " : j番目のパラメータ（重み）\n",
    "\n",
    "\n",
    "特徴量の数\n",
    "n\n",
    "は任意の値に対応できる実装にしてください。\n",
    "\n",
    "\n",
    "なお、ベクトル形式で表すと以下のようになります。\n",
    "\n",
    "$h_θ(x)=θ^T⋅x$\n",
    "\n",
    "クラスの外から呼び出すことがないメソッドのため、Pythonの慣例としてアンダースコアを先頭にひとつつけています。"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a53d1972",
   "metadata": {},
   "source": [
    "def _linear_hypothesis(self,X):\n",
    "    \"\"\"\n",
    "    線形の仮定関数を計算する\n",
    "    Parameters\n",
    "    ---------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "    訓練データ\n",
    "     Returns\n",
    "    -------\n",
    "    次の形のndarray, shape (n_samples, 1)\n",
    "    線形の仮定関数による推定結果\n",
    "    \"\"\"\n",
    "    self.theta = X.shape[1]\n",
    "\n",
    "    print(self.theta)\n",
    "    print(X)\n",
    "    y_hat = X @ self.theta\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ac5ac0a7-cd42-4173-85d5-713147f22742",
   "metadata": {},
   "source": [
    "#問題１、トイデータで練習\n",
    "X = np.arange(10)\n",
    "theta_n = X.shape[0]\n",
    "theta = np.ones((theta_n,1))\n",
    "print(\"theta\")    \n",
    "print(theta)\n",
    "print(theta.shape)\n",
    "print(\"X\")\n",
    "print(X)\n",
    "print(X.shape)\n",
    "\n",
    "y_hat =  X @ theta\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ad11bb",
   "metadata": {},
   "source": [
    "## 【問題2】最急降下法\n",
    "最急降下法により学習させる実装を行なってください。以下の式で表されるパラメータの更新式のメソッド_gradient_descentを追加し、fit\n",
    "メソッドから呼び出すようにしてください。\n",
    "\n",
    "$θ_j=θ_j−α\\frac{1}{m} \\sum_{i=1}^{m}[(h_θ(x^i)−y^i)x^i_j] \\quad$\n",
    "\n",
    "$\\alpha$ : 学習率\n",
    "\n",
    "$i$ : サンプルのインデックス\n",
    "\n",
    "\n",
    "$j$ : 特徴量のインデックス\n",
    "\n",
    "\n",
    "雛形\n",
    "\n",
    "\n",
    "ScratchLinearRegressionクラスへ以下のメソッドを追加してください。コメントアウト部分の説明も記述してください。\n",
    "\n",
    "#!\n",
    "[image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f7f8ded4",
   "metadata": {},
   "source": [
    "def _gradient_descent(self,X, error, theta, alpha):\n",
    "    \n",
    "    error = y_hat - y\n",
    "    alpha = 0.01\n",
    "    #下はメンターコード\n",
    "    #self.Theta = self.Theta - 0.01 * ((self.error.T * X).sum()) / x.shape[0]\n",
    "    self.theta = self.theta - alpha * (error.T @ X) / X.shape[0]\n",
    "    \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1d4ada71-0e79-484e-948d-e0af3b4742c8",
   "metadata": {},
   "source": [
    "#問題２トイデータ\n",
    "theta = np.array([[0,0]])\n",
    "print(theta.shape)\n",
    "\n",
    "x = np.linspace(1,6,5)\n",
    "y = 2*x + 1\n",
    "y = y.reshape(-1,1)\n",
    "X = np.c_[np.ones(5),x]\n",
    "#alpha = 0.01\n",
    "y_hat = X @ theta.T\n",
    "error = y_hat - y\n",
    "\n",
    "\n",
    "_gradient_descent(X, error, theta ,alpha)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fda7c5df",
   "metadata": {},
   "source": [
    "def gradient_descent():\n",
    "    pass\n",
    "def fit():\n",
    "    for i in range(iter_num):\n",
    "        gradient_descent()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a402b03-7bfe-4d03-9790-c77d65227d05",
   "metadata": {},
   "source": [
    "## 【問題3】推定\n",
    "推定する仕組みを実装してください。ScratchLinearRegressionクラスの雛形に含まれるpredictメソッドに書き加えてください。\n",
    "\n",
    "\n",
    "仮定関数 \n",
    "$hθ(x)$\n",
    " の出力が推定結果です。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058dfc0d",
   "metadata": {},
   "source": [
    "## 【問題4】平均二乗誤差\n",
    "線形回帰の指標値として用いられる平均二乗誤差（mean square error, MSE）の関数を作成してください。\n",
    "\n",
    "\n",
    "平均二乗誤差関数は回帰問題全般で使える関数のため、ScratchLinearRegressionクラスのメソッドではなく、別の関数として作成してください。雛形を用意してあります。\n",
    "\n",
    "\n",
    "平均二乗誤差は以下の数式で表されます。\n",
    "\n",
    "$L(\\theta)=  \\frac{1 }{ m}  \\sum_{i=1}^{m} (h_\\theta(x^{(i)})-y^{(i)})^{2}$\n",
    "\n",
    "\n",
    "m\n",
    " : 入力されるデータの数\n",
    "\n",
    "$\n",
    "h\n",
    "_\n",
    "θ\n",
    "(\n",
    ")\n",
    "$\n",
    " : 仮定関数\n",
    "\n",
    "$\n",
    "x\n",
    "(\n",
    "i\n",
    ")\n",
    "$\n",
    " : i番目のサンプルの特徴量ベクトル\n",
    "\n",
    "$\n",
    "y\n",
    "(\n",
    "i\n",
    ")\n",
    "$\n",
    " : i番目のサンプルの正解値\n",
    "\n",
    "\n",
    "なお、最急降下法のための目的関数（損失関数）としては、これを2で割ったものを使用します。（問題5, 9）\n",
    "\n",
    "\n",
    "雛形\n",
    "\n",
    "\n",
    "def MSE(y_pred, y):  \n",
    "    \"\"\"\n",
    "    平均二乗誤差の計算  \n",
    "    Parameters  \n",
    "    ----------\n",
    "    y_pred : 次の形のndarray, shape (n_samples,)  \n",
    "      推定した値  \n",
    "    y : 次の形のndarray, shape (n_samples,)  \n",
    "      正解値  \n",
    "    Returns  \n",
    "    ----------  \n",
    "    mse : numpy.float  \n",
    "      平均二乗誤差  \n",
    "    \"\"\"  \n",
    "    pass  \n",
    "    return mse"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8b19c05f",
   "metadata": {},
   "source": [
    "#平均二乗誤差の計算\n",
    "def _mse(y, y_hat):\n",
    "    mse = ((y_hat-y)**2).sum()/ (y.shape[0]) #len(y_hat)でもOK\n",
    "    \n",
    "    return mse  "
   ]
  },
  {
   "cell_type": "raw",
   "id": "e30bd8c2",
   "metadata": {},
   "source": [
    "#トイデータ\n",
    "y_hat = np.array([0,1,2,3,4,5])\n",
    "y = np.array([1,3,5,7,9,11])\n",
    "\n",
    "_mse(y_hat, y)\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "ba2562b0",
   "metadata": {},
   "source": [
    "# 検査用\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "y_pred = np.array([0,1,2,3,4,5])\n",
    "y = np.array([1,3,5,7,9,11])\n",
    "\n",
    "mean_squared_error(y_pred, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1805ab39",
   "metadata": {},
   "source": [
    "## 【問題5】目的関数\n",
    "以下の数式で表される線形回帰の 目的関数（損失関数） を実装してください。そして、これをself.loss, self.val_lossに記録するようにしてください。\n",
    "\n",
    "\n",
    "目的関数（損失関数） $J(θ)$  は次の式です。\n",
    "\n",
    "$J(\\theta)=  \\frac{1 }{ 2m}  \\sum_{i=1}^{m} (h_\\theta(x^{(i)})-y^{(i)})^2$\n",
    "\n",
    "\n",
    "m\n",
    " : 入力されるデータの数\n",
    "\n",
    "$\n",
    "h\n",
    "_\n",
    "θ\n",
    "(\n",
    ")\n",
    "$\n",
    " : 仮定関数\n",
    "\n",
    "$\n",
    "x\n",
    "(\n",
    "i\n",
    ")\n",
    "$\n",
    " : i番目のサンプルの特徴量ベクトル\n",
    "\n",
    "$\n",
    "y\n",
    "(\n",
    "i\n",
    ")\n",
    "$\n",
    " : i番目のサンプルの正解値"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e2b48c65",
   "metadata": {},
   "source": [
    "# 問題5 損失関数\n",
    "def _loss_func(self, y, y_hat):\n",
    "        \n",
    "    loss = self._mse(y, y_hat) / 2\n",
    "    return loss \n",
    "\n",
    "#トイデータ\n",
    "y_hat = np.array([0,1,2,3,4,5])\n",
    "y = np.array([1,3,5,7,9,11])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1785a340",
   "metadata": {},
   "source": [
    "## 【問題6】学習と推定\n",
    "機械学習スクラッチ入門のSprintで用意したHouse Pricesコンペティションのデータに対してスクラッチ実装の学習と推定を行なってください。\n",
    "\n",
    "\n",
    "scikit-learnによる実装と比べ、正しく動いているかを確認してください。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97c44ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (1460, 1) y: (1460, 1)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"train.csv\")\n",
    "df = pd.DataFrame(df)\n",
    "X = np.array(df[['YearBuilt']])\n",
    "y = np.array(df[[\"SalePrice\"]])\n",
    "print(\"X:\", X.shape, \"y:\", y.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "634ece20",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y.ravel(),test_size=0.20, random_state=0)\n",
    "print(f\"Xの訓練値:{x_train.shape}  Yの訓練値:{y_train.shape}  Xの正解値:{x_test.shape}  Yの正解値:{y_test.shape}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6e0d69a2",
   "metadata": {},
   "source": [
    "# 標準化\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "\n",
    "X_train_std = scaler.transform(X_train)\n",
    "X_test_std = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c1ead440",
   "metadata": {},
   "source": [
    "# sklearn線形回帰\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression().fit(X_train, y_train)\n",
    "lr_pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e0fdad36",
   "metadata": {},
   "source": [
    "plt.scatter(X_train_std, y_train, c=\"skyblue\")\n",
    "plt.scatter(X_test_std, lr_pred, c=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3fdf9799",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ScratchLinearRegression' object has no attribute 'theta'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-a71847e89ef4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mslr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mScratchLinearRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.00001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mno_bias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mslr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_std\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test_std\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mslr_pred_std\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_std\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-8fd0d631b56e>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, X_val, y_val)\u001b[0m\n\u001b[0;32m     51\u001b[0m         \"\"\"\n\u001b[0;32m     52\u001b[0m         \u001b[1;31m#問題１　仮定関数の計算\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_hat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_linear_hypothesis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[1;31m#問題２　最急降下法によるパラメータの更新値計算\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-8fd0d631b56e>\u001b[0m in \u001b[0;36m_linear_hypothesis\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[1;31m#print(X)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtheta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtheta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#init?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_hat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtheta\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ScratchLinearRegression' object has no attribute 'theta'"
     ]
    }
   ],
   "source": [
    "slr = ScratchLinearRegression(num_iter=200, lr=0.00001, no_bias=True ,verbose=False)\n",
    "slr.fit(X_train_std, y_train, X_test_std, y_test)\n",
    "slr_pred_std = slr.predict(X_test_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dd8a57",
   "metadata": {},
   "source": [
    "## 【問題7】学習曲線のプロット\n",
    "学習曲線を表示する関数を作成し、実行してください。グラフを見て損失が適切に下がっているかどうか確認してください。\n",
    "\n",
    "\n",
    "線形回帰クラスの雛形ではself.loss, self.val_lossに損失を記録しておくようになっているため、入力にはこれを利用してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61ae39c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": true,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

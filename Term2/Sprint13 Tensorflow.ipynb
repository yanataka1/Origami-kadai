{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint 13　TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ライブラリのimport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "%matplotlib inline\n",
    "# tensorflow1系\n",
    "# import tensorflow as tf\n",
    "# tensorflow2系\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 読み込み\n",
    "iris_dataset = load_iris()\n",
    "\n",
    "# 整形\n",
    "iris_dataframe = pd.DataFrame(data=iris_dataset.data, columns=iris_dataset.feature_names)\n",
    "iris_datalabel = pd.DataFrame(data=iris_dataset.target,columns=['Species'])\n",
    "df = pd.concat([iris_dataframe,iris_datalabel],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ミニバッチクラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetMiniBatch:\n",
    "\n",
    "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self.X[p0:p1], self.y[p0:p1]        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self.X[p0:p1], self.y[p0:p1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 問題2　スクラッチとTensorFlowの対応を考える"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データ準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-74baac333f80>:9: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  y = y.astype(np.int)[:, np.newaxis]\n"
     ]
    }
   ],
   "source": [
    "# 2値分類のため絞り込み\n",
    "df2 = df[(df[\"Species\"] == 0)|(df[\"Species\"] == 1)]\n",
    "\n",
    "# 説明変数と目的変数に分割\n",
    "y = df2[\"Species\"]\n",
    "X = df2.loc[:, [\"sepal length (cm)\", \"sepal width (cm)\", \"petal length (cm)\", \"petal width (cm)\"]]\n",
    "y = np.array(y)\n",
    "X = np.array(X)\n",
    "y = y.astype(np.int)[:, np.newaxis]\n",
    "\n",
    "# 訓練データ/テストデータ/評価データに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "# 正規化\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "X_val /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensorflowで学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-435ce693509b>:9: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train_loss : 3.3061, val_loss : 4.3819, train_acc : 0.531, val_acc : 0.375\n",
      "Epoch 1, train_loss : 1.0542, val_loss : 0.7399, train_acc : 0.469, val_acc : 0.625\n",
      "Epoch 2, train_loss : 0.6860, val_loss : 0.4991, train_acc : 0.469, val_acc : 0.625\n",
      "Epoch 3, train_loss : 0.6305, val_loss : 0.7858, train_acc : 0.531, val_acc : 0.375\n",
      "Epoch 4, train_loss : 0.3821, val_loss : 0.3793, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 5, train_loss : 0.7725, val_loss : 0.5373, train_acc : 0.469, val_acc : 0.625\n",
      "Epoch 6, train_loss : 0.7720, val_loss : 0.5322, train_acc : 0.469, val_acc : 0.625\n",
      "Epoch 7, train_loss : 0.4598, val_loss : 0.3315, train_acc : 0.688, val_acc : 0.875\n",
      "Epoch 8, train_loss : 0.3537, val_loss : 0.2687, train_acc : 0.922, val_acc : 0.938\n",
      "Epoch 9, train_loss : 0.3449, val_loss : 0.2552, train_acc : 0.906, val_acc : 0.938\n",
      "Epoch 10, train_loss : 0.3712, val_loss : 0.2613, train_acc : 0.828, val_acc : 0.938\n",
      "Epoch 11, train_loss : 0.3854, val_loss : 0.2636, train_acc : 0.781, val_acc : 0.875\n",
      "Epoch 12, train_loss : 0.3596, val_loss : 0.2435, train_acc : 0.828, val_acc : 0.875\n",
      "Epoch 13, train_loss : 0.3055, val_loss : 0.2080, train_acc : 0.906, val_acc : 0.938\n",
      "Epoch 14, train_loss : 0.2509, val_loss : 0.1744, train_acc : 0.938, val_acc : 1.000\n",
      "Epoch 15, train_loss : 0.2126, val_loss : 0.1518, train_acc : 0.969, val_acc : 1.000\n",
      "Epoch 16, train_loss : 0.1914, val_loss : 0.1380, train_acc : 0.984, val_acc : 1.000\n",
      "Epoch 17, train_loss : 0.1787, val_loss : 0.1281, train_acc : 0.984, val_acc : 1.000\n",
      "Epoch 18, train_loss : 0.1676, val_loss : 0.1193, train_acc : 0.984, val_acc : 1.000\n",
      "Epoch 19, train_loss : 0.1548, val_loss : 0.1106, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 20, train_loss : 0.1423, val_loss : 0.1027, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 21, train_loss : 0.1322, val_loss : 0.0960, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 22, train_loss : 0.1243, val_loss : 0.0902, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 23, train_loss : 0.1171, val_loss : 0.0849, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 24, train_loss : 0.1102, val_loss : 0.0800, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 25, train_loss : 0.1038, val_loss : 0.0756, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 26, train_loss : 0.0983, val_loss : 0.0715, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 27, train_loss : 0.0933, val_loss : 0.0678, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 28, train_loss : 0.0886, val_loss : 0.0644, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 29, train_loss : 0.0843, val_loss : 0.0612, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 30, train_loss : 0.0803, val_loss : 0.0583, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 31, train_loss : 0.0766, val_loss : 0.0555, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 32, train_loss : 0.0733, val_loss : 0.0529, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 33, train_loss : 0.0701, val_loss : 0.0506, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 34, train_loss : 0.0671, val_loss : 0.0483, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 35, train_loss : 0.0644, val_loss : 0.0462, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 36, train_loss : 0.0617, val_loss : 0.0443, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 37, train_loss : 0.0593, val_loss : 0.0424, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 38, train_loss : 0.0570, val_loss : 0.0406, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 39, train_loss : 0.0549, val_loss : 0.0390, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 40, train_loss : 0.0528, val_loss : 0.0374, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 41, train_loss : 0.0509, val_loss : 0.0360, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 42, train_loss : 0.0491, val_loss : 0.0346, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 43, train_loss : 0.0474, val_loss : 0.0333, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 44, train_loss : 0.0457, val_loss : 0.0320, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 45, train_loss : 0.0442, val_loss : 0.0308, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 46, train_loss : 0.0427, val_loss : 0.0297, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 47, train_loss : 0.0413, val_loss : 0.0286, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 48, train_loss : 0.0400, val_loss : 0.0276, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 49, train_loss : 0.0387, val_loss : 0.0267, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 50, train_loss : 0.0375, val_loss : 0.0257, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 51, train_loss : 0.0364, val_loss : 0.0249, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 52, train_loss : 0.0353, val_loss : 0.0240, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 53, train_loss : 0.0342, val_loss : 0.0232, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 54, train_loss : 0.0332, val_loss : 0.0225, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 55, train_loss : 0.0323, val_loss : 0.0217, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 56, train_loss : 0.0314, val_loss : 0.0210, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 57, train_loss : 0.0305, val_loss : 0.0204, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 58, train_loss : 0.0296, val_loss : 0.0197, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 59, train_loss : 0.0288, val_loss : 0.0191, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 60, train_loss : 0.0281, val_loss : 0.0185, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 61, train_loss : 0.0273, val_loss : 0.0180, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 62, train_loss : 0.0266, val_loss : 0.0174, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 63, train_loss : 0.0259, val_loss : 0.0169, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 64, train_loss : 0.0252, val_loss : 0.0164, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 65, train_loss : 0.0246, val_loss : 0.0159, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 66, train_loss : 0.0240, val_loss : 0.0155, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 67, train_loss : 0.0234, val_loss : 0.0150, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 68, train_loss : 0.0228, val_loss : 0.0146, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 69, train_loss : 0.0223, val_loss : 0.0142, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 70, train_loss : 0.0217, val_loss : 0.0138, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 71, train_loss : 0.0212, val_loss : 0.0134, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 72, train_loss : 0.0207, val_loss : 0.0131, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 73, train_loss : 0.0202, val_loss : 0.0127, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 74, train_loss : 0.0198, val_loss : 0.0124, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 75, train_loss : 0.0193, val_loss : 0.0121, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 76, train_loss : 0.0189, val_loss : 0.0118, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 77, train_loss : 0.0185, val_loss : 0.0114, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 78, train_loss : 0.0181, val_loss : 0.0112, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 79, train_loss : 0.0177, val_loss : 0.0109, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 80, train_loss : 0.0173, val_loss : 0.0106, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 81, train_loss : 0.0169, val_loss : 0.0103, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 82, train_loss : 0.0166, val_loss : 0.0101, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 83, train_loss : 0.0162, val_loss : 0.0098, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 84, train_loss : 0.0159, val_loss : 0.0096, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 85, train_loss : 0.0156, val_loss : 0.0094, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 86, train_loss : 0.0153, val_loss : 0.0092, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 87, train_loss : 0.0150, val_loss : 0.0089, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 88, train_loss : 0.0147, val_loss : 0.0087, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 89, train_loss : 0.0144, val_loss : 0.0085, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 90, train_loss : 0.0141, val_loss : 0.0083, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 91, train_loss : 0.0138, val_loss : 0.0081, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 92, train_loss : 0.0136, val_loss : 0.0080, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 93, train_loss : 0.0133, val_loss : 0.0078, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 94, train_loss : 0.0131, val_loss : 0.0076, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 95, train_loss : 0.0128, val_loss : 0.0074, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 96, train_loss : 0.0126, val_loss : 0.0073, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 97, train_loss : 0.0123, val_loss : 0.0071, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 98, train_loss : 0.0121, val_loss : 0.0070, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 99, train_loss : 0.0119, val_loss : 0.0068, train_acc : 1.000, val_acc : 1.000\n",
      "test_acc : 1.000\n"
     ]
    }
   ],
   "source": [
    "# 各種変数定義\n",
    "learning_rate = 0.01\n",
    "batch_size = 10\n",
    "num_epochs = 100\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 1\n",
    "\n",
    "# 空配列定義\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# ミニバッチイテレータ生成\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
    "\n",
    "def example_net(x):\n",
    "    \"\"\"tensorflowを利用したニューラルネットワーク\n",
    "    Parameters\n",
    "    ---------------\n",
    "    x : 入力配列\n",
    "    \"\"\"\n",
    "    # 重み定義\n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    # バイアス定義\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "    # 計算グラフ構築（順伝播処理）\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3']\n",
    "    return layer_output\n",
    "\n",
    "# 計算グラフ受け取る\n",
    "logits = example_net(X)\n",
    "# 損失定義\n",
    "loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n",
    "# 最適化手法の定義\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "# 最適化手法で、定義した損失を最小化するルールを作成\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "# ACC計算\n",
    "correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# 変数を扱うためのおまじない\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# tensorflowのセッション開始\n",
    "with tf.Session() as sess:\n",
    "    # 初期化\n",
    "    sess.run(init)\n",
    "    # 学習回数分ループ\n",
    "    for epoch in range(num_epochs):\n",
    "        # ミニバッチイテレータでループ\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            # 「最適化手法で、定義した損失を最小化するルール」を実行\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "        # 記録\n",
    "        train_loss, train_acc = sess.run([loss_op, accuracy], feed_dict={X: X_train, Y: y_train})\n",
    "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
    "        # 仮定出力\n",
    "        print(\"Epoch {}, train_loss : {:.4f}, val_loss : {:.4f}, train_acc : {:.3f}, val_acc : {:.3f}\".format(epoch, train_loss, val_loss, train_acc, val_acc))\n",
    "    \n",
    "    # 学習が終了したらテストデータで実行\n",
    "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
    "    #prediction = sess.run(logits, feed_dict={X: X_test, Y: y_test})\n",
    "    #print(prediction)\n",
    "    print(\"test_acc : {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 問題3　3種類すべての目的変数を使用したIrisのモデルを作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データ準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-2db749e1cff2>:6: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  y = y.astype(np.int)[:, np.newaxis]\n"
     ]
    }
   ],
   "source": [
    "# 説明変数と目的変数に分割\n",
    "y = df[\"Species\"]\n",
    "X = df.loc[:, [\"sepal length (cm)\", \"sepal width (cm)\", \"petal length (cm)\", \"petal width (cm)\"]]\n",
    "y = np.array(y)\n",
    "X = np.array(X)\n",
    "y = y.astype(np.int)[:, np.newaxis]\n",
    "\n",
    "# 訓練データ/テストデータ/評価データに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "# onehotベクトル化\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train)\n",
    "y_val_one_hot = enc.transform(y_val)\n",
    "y_test_one_hot = enc.transform(y_test)\n",
    "\n",
    "# 正規化\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "X_val /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensorflowで学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-435ce693509b>:9: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train_loss : 9.7459, val_loss : 9.6208, train_acc : 0.365, val_acc : 0.375\n",
      "Epoch 1, train_loss : 7.1815, val_loss : 7.3793, train_acc : 0.312, val_acc : 0.292\n",
      "Epoch 2, train_loss : 1.1437, val_loss : 1.1052, train_acc : 0.323, val_acc : 0.333\n",
      "Epoch 3, train_loss : 1.2079, val_loss : 1.1626, train_acc : 0.323, val_acc : 0.333\n",
      "Epoch 4, train_loss : 1.1498, val_loss : 1.1344, train_acc : 0.333, val_acc : 0.333\n",
      "Epoch 5, train_loss : 1.1417, val_loss : 1.0888, train_acc : 0.323, val_acc : 0.333\n",
      "Epoch 6, train_loss : 1.3163, val_loss : 1.2361, train_acc : 0.344, val_acc : 0.333\n",
      "Epoch 7, train_loss : 1.1172, val_loss : 1.0468, train_acc : 0.688, val_acc : 0.708\n",
      "Epoch 8, train_loss : 0.9346, val_loss : 0.8852, train_acc : 0.688, val_acc : 0.708\n",
      "Epoch 9, train_loss : 0.7877, val_loss : 0.7643, train_acc : 0.688, val_acc : 0.708\n",
      "Epoch 10, train_loss : 0.7770, val_loss : 0.7634, train_acc : 0.698, val_acc : 0.708\n",
      "Epoch 11, train_loss : 0.7578, val_loss : 0.7385, train_acc : 0.688, val_acc : 0.708\n",
      "Epoch 12, train_loss : 0.7212, val_loss : 0.7032, train_acc : 0.688, val_acc : 0.708\n",
      "Epoch 13, train_loss : 0.6959, val_loss : 0.6782, train_acc : 0.688, val_acc : 0.708\n",
      "Epoch 14, train_loss : 0.6748, val_loss : 0.6556, train_acc : 0.688, val_acc : 0.708\n",
      "Epoch 15, train_loss : 0.6512, val_loss : 0.6331, train_acc : 0.688, val_acc : 0.708\n",
      "Epoch 16, train_loss : 0.6312, val_loss : 0.6146, train_acc : 0.708, val_acc : 0.708\n",
      "Epoch 17, train_loss : 0.6121, val_loss : 0.5974, train_acc : 0.719, val_acc : 0.708\n",
      "Epoch 18, train_loss : 0.5943, val_loss : 0.5820, train_acc : 0.792, val_acc : 0.708\n",
      "Epoch 19, train_loss : 0.5781, val_loss : 0.5680, train_acc : 0.865, val_acc : 0.792\n",
      "Epoch 20, train_loss : 0.5631, val_loss : 0.5552, train_acc : 0.896, val_acc : 0.833\n",
      "Epoch 21, train_loss : 0.5489, val_loss : 0.5431, train_acc : 0.917, val_acc : 0.833\n",
      "Epoch 22, train_loss : 0.5354, val_loss : 0.5313, train_acc : 0.948, val_acc : 0.875\n",
      "Epoch 23, train_loss : 0.5223, val_loss : 0.5197, train_acc : 0.958, val_acc : 0.875\n",
      "Epoch 24, train_loss : 0.5095, val_loss : 0.5081, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 25, train_loss : 0.4968, val_loss : 0.4966, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 26, train_loss : 0.4844, val_loss : 0.4851, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 27, train_loss : 0.4723, val_loss : 0.4738, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 28, train_loss : 0.4604, val_loss : 0.4627, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 29, train_loss : 0.4490, val_loss : 0.4519, train_acc : 0.969, val_acc : 0.917\n",
      "Epoch 30, train_loss : 0.4379, val_loss : 0.4414, train_acc : 0.979, val_acc : 0.917\n",
      "Epoch 31, train_loss : 0.4272, val_loss : 0.4314, train_acc : 0.979, val_acc : 0.917\n",
      "Epoch 32, train_loss : 0.4169, val_loss : 0.4217, train_acc : 0.979, val_acc : 0.958\n",
      "Epoch 33, train_loss : 0.4070, val_loss : 0.4124, train_acc : 0.979, val_acc : 0.917\n",
      "Epoch 34, train_loss : 0.3975, val_loss : 0.4034, train_acc : 0.979, val_acc : 0.917\n",
      "Epoch 35, train_loss : 0.3883, val_loss : 0.3948, train_acc : 0.979, val_acc : 0.917\n",
      "Epoch 36, train_loss : 0.3795, val_loss : 0.3866, train_acc : 0.969, val_acc : 0.917\n",
      "Epoch 37, train_loss : 0.3710, val_loss : 0.3787, train_acc : 0.969, val_acc : 0.917\n",
      "Epoch 38, train_loss : 0.3628, val_loss : 0.3711, train_acc : 0.969, val_acc : 0.917\n",
      "Epoch 39, train_loss : 0.3548, val_loss : 0.3639, train_acc : 0.969, val_acc : 0.917\n",
      "Epoch 40, train_loss : 0.3471, val_loss : 0.3570, train_acc : 0.969, val_acc : 0.917\n",
      "Epoch 41, train_loss : 0.3394, val_loss : 0.3505, train_acc : 0.969, val_acc : 0.917\n",
      "Epoch 42, train_loss : 0.3317, val_loss : 0.3443, train_acc : 0.969, val_acc : 0.917\n",
      "Epoch 43, train_loss : 0.3240, val_loss : 0.3385, train_acc : 0.969, val_acc : 0.917\n",
      "Epoch 44, train_loss : 0.3164, val_loss : 0.3331, train_acc : 0.969, val_acc : 0.917\n",
      "Epoch 45, train_loss : 0.3090, val_loss : 0.3279, train_acc : 0.969, val_acc : 0.958\n",
      "Epoch 46, train_loss : 0.3020, val_loss : 0.3229, train_acc : 0.969, val_acc : 0.917\n",
      "Epoch 47, train_loss : 0.2952, val_loss : 0.3179, train_acc : 0.979, val_acc : 0.917\n",
      "Epoch 48, train_loss : 0.2886, val_loss : 0.3129, train_acc : 0.979, val_acc : 0.917\n",
      "Epoch 49, train_loss : 0.2822, val_loss : 0.3080, train_acc : 0.979, val_acc : 0.917\n",
      "Epoch 50, train_loss : 0.2760, val_loss : 0.3032, train_acc : 0.979, val_acc : 0.917\n",
      "Epoch 51, train_loss : 0.2700, val_loss : 0.2987, train_acc : 0.969, val_acc : 0.917\n",
      "Epoch 52, train_loss : 0.2641, val_loss : 0.2943, train_acc : 0.969, val_acc : 0.917\n",
      "Epoch 53, train_loss : 0.2584, val_loss : 0.2901, train_acc : 0.969, val_acc : 0.917\n",
      "Epoch 54, train_loss : 0.2529, val_loss : 0.2861, train_acc : 0.969, val_acc : 0.917\n",
      "Epoch 55, train_loss : 0.2476, val_loss : 0.2823, train_acc : 0.969, val_acc : 0.917\n",
      "Epoch 56, train_loss : 0.2424, val_loss : 0.2786, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 57, train_loss : 0.2373, val_loss : 0.2750, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 58, train_loss : 0.2325, val_loss : 0.2716, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 59, train_loss : 0.2278, val_loss : 0.2683, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 60, train_loss : 0.2233, val_loss : 0.2653, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 61, train_loss : 0.2189, val_loss : 0.2622, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 62, train_loss : 0.2146, val_loss : 0.2595, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 63, train_loss : 0.2105, val_loss : 0.2567, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 64, train_loss : 0.2066, val_loss : 0.2542, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 65, train_loss : 0.2027, val_loss : 0.2515, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 66, train_loss : 0.1990, val_loss : 0.2493, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 67, train_loss : 0.1954, val_loss : 0.2470, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 68, train_loss : 0.1920, val_loss : 0.2448, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 69, train_loss : 0.1886, val_loss : 0.2427, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 70, train_loss : 0.1854, val_loss : 0.2408, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 71, train_loss : 0.1823, val_loss : 0.2387, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 72, train_loss : 0.1793, val_loss : 0.2372, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 73, train_loss : 0.1763, val_loss : 0.2351, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 74, train_loss : 0.1736, val_loss : 0.2337, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 75, train_loss : 0.1708, val_loss : 0.2321, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 76, train_loss : 0.1682, val_loss : 0.2305, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 77, train_loss : 0.1656, val_loss : 0.2291, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 78, train_loss : 0.1631, val_loss : 0.2276, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 79, train_loss : 0.1607, val_loss : 0.2263, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 80, train_loss : 0.1585, val_loss : 0.2252, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 81, train_loss : 0.1562, val_loss : 0.2238, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 82, train_loss : 0.1540, val_loss : 0.2226, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 83, train_loss : 0.1519, val_loss : 0.2215, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 84, train_loss : 0.1499, val_loss : 0.2206, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 85, train_loss : 0.1479, val_loss : 0.2195, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 86, train_loss : 0.1459, val_loss : 0.2183, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 87, train_loss : 0.1441, val_loss : 0.2175, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 88, train_loss : 0.1423, val_loss : 0.2167, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 89, train_loss : 0.1406, val_loss : 0.2158, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 90, train_loss : 0.1388, val_loss : 0.2147, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 91, train_loss : 0.1372, val_loss : 0.2141, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 92, train_loss : 0.1356, val_loss : 0.2134, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 93, train_loss : 0.1340, val_loss : 0.2126, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 94, train_loss : 0.1325, val_loss : 0.2119, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 95, train_loss : 0.1311, val_loss : 0.2113, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 96, train_loss : 0.1295, val_loss : 0.2105, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 97, train_loss : 0.1282, val_loss : 0.2101, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 98, train_loss : 0.1268, val_loss : 0.2092, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 99, train_loss : 0.1256, val_loss : 0.2090, train_acc : 0.958, val_acc : 0.917\n",
      "test_acc : 1.000\n"
     ]
    }
   ],
   "source": [
    "# 各種変数定義\n",
    "learning_rate = 0.01\n",
    "batch_size = 10\n",
    "num_epochs = 100\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 3 # 2値分類からの変更箇所\n",
    "\n",
    "# 空配列定義\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# ミニバッチイテレータ生成\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train_one_hot, batch_size=batch_size)\n",
    "\n",
    "def example_net(x):\n",
    "    \"\"\"tensorflowを利用したニューラルネットワーク\n",
    "    Parameters\n",
    "    ---------------\n",
    "    x : 入力配列\n",
    "    \"\"\"\n",
    "    # 重み定義\n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    # バイアス定義\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "    # 計算グラフ構築（順伝播処理）\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3']\n",
    "    return layer_output\n",
    "\n",
    "# 計算グラフ受け取る\n",
    "logits = example_net(X)\n",
    "# 損失定義\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits)) # 2値分類からの変更箇所\n",
    "# 最適化手法の定義\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "# 最適化手法で、定義した損失を最小化するルールを作成\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "# ACC計算\n",
    "correct_pred = tf.equal(tf.argmax(Y, 1), tf.argmax(logits, 1)) # 2値分類からの変更箇所\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# 変数を扱うためのおまじない\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# tensorflowのセッション開始\n",
    "with tf.Session() as sess:\n",
    "    # 初期化\n",
    "    sess.run(init)\n",
    "    # 学習回数分ループ\n",
    "    for epoch in range(num_epochs):\n",
    "        # ミニバッチイテレータでループ\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            # 「最適化手法で、定義した損失を最小化するルール」を実行\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "        # 記録\n",
    "        train_loss, train_acc = sess.run([loss_op, accuracy], feed_dict={X: X_train, Y: y_train_one_hot})\n",
    "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val_one_hot})\n",
    "        # 仮定出力\n",
    "        print(\"Epoch {}, train_loss : {:.4f}, val_loss : {:.4f}, train_acc : {:.3f}, val_acc : {:.3f}\".format(epoch, train_loss, val_loss, train_acc, val_acc))\n",
    "    \n",
    "    # 学習が終了したらテストデータで実行\n",
    "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test_one_hot})\n",
    "    #prediction = sess.run(logits, feed_dict={X: X_test, Y: y_test})\n",
    "    #print(prediction)\n",
    "    print(\"test_acc : {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 問題4　House Pricesのモデルを作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データ準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-1c57680061e4>:8: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  y = y.astype(np.int)[:, np.newaxis]\n"
     ]
    }
   ],
   "source": [
    "dataset_path =\"train.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "y = df[\"SalePrice\"]\n",
    "X = df.loc[:, [\"GrLivArea\", \"YearBuilt\"]]\n",
    "y = np.array(y)\n",
    "X = np.array(X)\n",
    "y = y.astype(np.int)[:, np.newaxis]\n",
    "y = np.log(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensorflowで学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-435ce693509b>:9: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss : 126624456.0000, val_loss : 130158600.0000\n",
      "Epoch 1, loss : 19655940.0000, val_loss : 13549530.0000\n",
      "Epoch 2, loss : 7223943.5000, val_loss : 3114283.5000\n",
      "Epoch 3, loss : 4064055.2500, val_loss : 1483210.6250\n",
      "Epoch 4, loss : 2634586.2500, val_loss : 894731.6250\n",
      "Epoch 5, loss : 1892756.2500, val_loss : 610601.5625\n",
      "Epoch 6, loss : 1383819.2500, val_loss : 455188.0938\n",
      "Epoch 7, loss : 1057081.8750, val_loss : 309920.7812\n",
      "Epoch 8, loss : 810914.3750, val_loss : 238962.5000\n",
      "Epoch 9, loss : 643254.2500, val_loss : 179425.0938\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_mse : 3603451.250\n"
     ]
    }
   ],
   "source": [
    "# 各種変数定義\n",
    "learning_rate = 0.001\n",
    "batch_size = 10\n",
    "num_epochs = 10\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 1\n",
    "\n",
    "# 空配列定義\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# ミニバッチイテレータ生成\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
    "\n",
    "def example_net(x):\n",
    "    \"\"\"tensorflowを利用したニューラルネットワーク\n",
    "    Parameters\n",
    "    ---------------\n",
    "    x : 入力配列\n",
    "    \"\"\"\n",
    "    # 重み定義\n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    # バイアス定義\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "    # 計算グラフ構築（順伝播処理）\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3']\n",
    "    return layer_output\n",
    "\n",
    "# 計算グラフ受け取る\n",
    "logits = example_net(X)\n",
    "# 損失定義\n",
    "loss_op =  tf.losses.mean_squared_error(labels=Y, predictions=logits)\n",
    "# 最適化手法の定義\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "# 最適化手法で、定義した損失を最小化するルールを作成\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# 変数を扱うためのおまじない\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# tensorflowのセッション開始\n",
    "with tf.Session() as sess:\n",
    "    # 初期化\n",
    "    sess.run(init)\n",
    "    # 損失記録用リスト\n",
    "    loss_list = []\n",
    "    val_loss_list = []\n",
    "    # 学習回数分ループ\n",
    "    for epoch in range(num_epochs):\n",
    "        # ミニバッチイテレータでループ\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            # 「最適化手法で、定義した損失を最小化するルール」を実行\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "        # 損失計算と格納\n",
    "        loss = sess.run(loss_op, feed_dict={X: X_train, Y: y_train})\n",
    "        val_loss = sess.run(loss_op, feed_dict={X: X_val, Y: y_val})\n",
    "        loss_list.append(loss)\n",
    "        val_loss_list.append(val_loss)    \n",
    "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}\".format(epoch, loss, val_loss))\n",
    "    \n",
    "    # 学習過程可視化\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.plot(loss_list, label='loss')\n",
    "    plt.plot(val_loss_list, label='val_loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # テストデータに適用\n",
    "    test_loss = sess.run(loss_op, feed_dict={X: X_test, Y: y_test})\n",
    "    print(\"test_mse : {:.3f}\".format(test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 問題5　MNISTのモデルを作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データ準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-3971c584dfe3>:9: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  X_train = X_train.astype(np.float)\n",
      "<ipython-input-10-3971c584dfe3>:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  X_test = X_test.astype(np.float)\n",
      "<ipython-input-10-3971c584dfe3>:15: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  y_train = y_train.astype(np.int)[:, np.newaxis]\n",
      "<ipython-input-10-3971c584dfe3>:16: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  y_test = y_test.astype(np.int)[:, np.newaxis]\n"
     ]
    }
   ],
   "source": [
    "# 読み込み\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "#　平滑化\n",
    "X_train = X_train.reshape(-1, 784)\n",
    "X_test = X_test.reshape(-1, 784)\n",
    "\n",
    "# 正規化\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# 変形\n",
    "y_train = y_train.astype(np.int)[:, np.newaxis]\n",
    "y_test = y_test.astype(np.int)[:, np.newaxis]\n",
    "\n",
    "# one-hotベクトル化\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train[:])\n",
    "y_test_one_hot = enc.fit_transform(y_test[:])\n",
    "\n",
    "# 分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train_one_hot, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensorflowで学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-435ce693509b>:9: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train_loss : 35.8685, val_loss : 35.1439, train_acc : 0.536, val_acc : 0.532\n",
      "Epoch 1, train_loss : 23.0694, val_loss : 22.4763, train_acc : 0.648, val_acc : 0.652\n",
      "Epoch 2, train_loss : 20.0082, val_loss : 19.5707, train_acc : 0.675, val_acc : 0.677\n",
      "Epoch 3, train_loss : 18.5318, val_loss : 18.5954, train_acc : 0.705, val_acc : 0.701\n",
      "Epoch 4, train_loss : 18.5639, val_loss : 18.3858, train_acc : 0.720, val_acc : 0.720\n",
      "Epoch 5, train_loss : 17.0441, val_loss : 16.7574, train_acc : 0.741, val_acc : 0.744\n",
      "Epoch 6, train_loss : 16.7718, val_loss : 16.6835, train_acc : 0.749, val_acc : 0.745\n",
      "Epoch 7, train_loss : 14.8467, val_loss : 14.3798, train_acc : 0.768, val_acc : 0.771\n",
      "Epoch 8, train_loss : 16.3406, val_loss : 15.9803, train_acc : 0.784, val_acc : 0.786\n",
      "Epoch 9, train_loss : 15.0752, val_loss : 14.5005, train_acc : 0.799, val_acc : 0.798\n",
      "Epoch 10, train_loss : 17.4885, val_loss : 17.2704, train_acc : 0.789, val_acc : 0.788\n",
      "Epoch 11, train_loss : 16.0757, val_loss : 15.5926, train_acc : 0.812, val_acc : 0.812\n",
      "Epoch 12, train_loss : 14.5812, val_loss : 13.9689, train_acc : 0.818, val_acc : 0.819\n",
      "Epoch 13, train_loss : 16.6584, val_loss : 16.3426, train_acc : 0.821, val_acc : 0.819\n",
      "Epoch 14, train_loss : 16.5475, val_loss : 15.9630, train_acc : 0.820, val_acc : 0.825\n",
      "Epoch 15, train_loss : 16.5127, val_loss : 15.8654, train_acc : 0.815, val_acc : 0.818\n",
      "Epoch 16, train_loss : 19.9407, val_loss : 19.6147, train_acc : 0.803, val_acc : 0.802\n",
      "Epoch 17, train_loss : 17.8301, val_loss : 17.3559, train_acc : 0.815, val_acc : 0.818\n",
      "Epoch 18, train_loss : 16.9564, val_loss : 16.6006, train_acc : 0.822, val_acc : 0.821\n",
      "Epoch 19, train_loss : 15.5214, val_loss : 15.2969, train_acc : 0.824, val_acc : 0.824\n",
      "test_acc : 0.821\n"
     ]
    }
   ],
   "source": [
    "# 各種変数定義\n",
    "learning_rate = 0.003\n",
    "batch_size = 1\n",
    "num_epochs = 20\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 10 # 2値分類からの変更箇所\n",
    "\n",
    "# 空配列定義\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# ミニバッチイテレータ生成\n",
    "get_mini_batch_train = GetMiniBatch(X_train[:1000], y_train[:1000], batch_size=batch_size)\n",
    "\n",
    "def example_net(x):\n",
    "    \"\"\"tensorflowを利用したニューラルネットワーク\n",
    "    Parameters\n",
    "    ---------------\n",
    "    x : 入力配列\n",
    "    \"\"\"\n",
    "    # 重み定義\n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    # バイアス定義\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "    # 計算グラフ構築（順伝播処理）\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3']\n",
    "    return layer_output\n",
    "\n",
    "# 計算グラフ受け取る\n",
    "logits = example_net(X)\n",
    "# 損失定義\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits)) # 2値分類からの変更箇所\n",
    "# 最適化手法の定義\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "# 最適化手法で、定義した損失を最小化するルールを作成\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "# ACC計算\n",
    "correct_pred = tf.equal(tf.argmax(Y, 1), tf.argmax(logits, 1)) # 2値分類からの変更箇所\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# 変数を扱うためのおまじない\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# tensorflowのセッション開始\n",
    "with tf.Session() as sess:\n",
    "    # 初期化\n",
    "    sess.run(init)\n",
    "    # 学習回数分ループ\n",
    "    for epoch in range(num_epochs):\n",
    "        # ミニバッチイテレータでループ\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            # 「最適化手法で、定義した損失を最小化するルール」を実行\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "        # 記録\n",
    "        train_loss, train_acc = sess.run([loss_op, accuracy], feed_dict={X: X_train, Y: y_train})\n",
    "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
    "        # 仮定出力\n",
    "        print(\"Epoch {}, train_loss : {:.4f}, val_loss : {:.4f}, train_acc : {:.3f}, val_acc : {:.3f}\".format(epoch, train_loss, val_loss, train_acc, val_acc))\n",
    "    \n",
    "    # 学習が終了したらテストデータで実行\n",
    "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test_one_hot})\n",
    "    #prediction = sess.run(logits, feed_dict={X: X_test, Y: y_test})\n",
    "    #print(prediction)\n",
    "    print(\"test_acc : {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

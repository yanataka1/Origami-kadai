{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a095b884",
   "metadata": {},
   "source": [
    "# ヒント集【Sprint 機械学習スクラッチ ロジスティック回帰】\n",
    "## ゴール\n",
    "- **【Sprint 機械学習スクラッチ ロジスティック回帰】を解くうえで必要な知識や技術について理解できる**\n",
    "\n",
    "## このヒント集について\n",
    "### Sprintの目的\n",
    "- ロジスティック回帰の数学的知識について、理解する\n",
    "- ロジスティック回帰をスクラッチ実装する際のより踏み込んだヒントを掲載しているので、それを基に、スクラッチ実装できるようになる。\n",
    "\n",
    "### どのように学ぶか\n",
    "\n",
    "【Sprint 機械学習スクラッチ ロジスティック回帰】の目次と照らし合わせながら、進めていきましょう。\n",
    "\n",
    "## ロジスティック回帰\n",
    "### ロジスティック回帰とは\n",
    "主に生存分析（生きるか死ぬか）などで用いられるアルゴリズムです。生存分析は、病気や医療の領域だけでなく、広義には、ユーザーが特定の行動を起こすか否かというような問題にも適用され、様々な領域・分野で用いられています。\n",
    "\n",
    "つまり、簡単に言うと、**事象が起こる確率を算出する**アルゴリズムです。\n",
    "\n",
    "もう少し詳しく、数学的な側面からご紹介いたします。\n",
    "\n",
    "**ロジスティック回帰(Logistic regression)**とは、ベルヌーイ分布 / 二項分布に従う分類モデルの一種である。\n",
    "\n",
    "ロジスティック回帰の出力は、「ある出来事が発生する確率」であり、この出力によってクラスの分類を行う。\n",
    "\n",
    "この確率は、平均へと回帰する線形結合を「関数で変換する」ことによって導かれるため、ロジスティック回帰は分類問題のタスクに用いられるにも関わらず、「回帰」と名付けられている。\n",
    "\n",
    "具体的には、線形回帰の出力をあとで紹介するシグモイド関数に通して二項分布の正規確率へと変換したものがロジスティック回帰の出力である。\n",
    "\n",
    "ゆえに、ロジスティック回帰も一方の変数を他方の変数(要因ごとに重みをかけた上で)によって説明しようとする点は線形回帰と変わりはない。\n",
    "\n",
    "### ロジスティック回帰のイメージ\n",
    "ロジスティック回帰は、 隠れ層のないニューラルネットワークと同じ構造であることから、ロジスティック回帰の理解は、深層学習への橋渡しとなる。\n",
    "\n",
    "### ロジスティック回帰の前提\n",
    "1. 説明変数(x)は連続値 or 離散値で、目的変数(y)は離散値である。\n",
    "2. 予測値()は、ベルヌーイ分布 / 二項分布(平均np、分散np(1-p))に従う。\n",
    "\n",
    "### 二項分布\n",
    "互いに独立したベルヌーイ試行(2種類のみの結果しか得られないような試行。例えば、「サイコロを投げた場合1なのか、それ以外なのか?」というのを考える場合はベルヌーイ試行だが、「サイコロを投げてどの目が出るか?」というのを考えるのはベルヌーイ試行ではない)を独立にn回行ったときに、ある事象が何回起こるか(成功回数)を表す離散確率分布のことである。パラメータとして試行回数と成功確率を持つ。二項分布はnが十分に大きいとき、平均np、分散np(1-p)の正規分布に近づく(pは成功確率のこと)。試行回数が1回の二項分布は、べルヌーイ分布に等しい。\n",
    "\n",
    "2項分布とベルヌーイ分布のグラフと式（期待値・分散・標準偏差・確率質量関数）\n",
    "\n",
    "## 尤度関数\n",
    "\n",
    "ベルヌーイ分布の尤度関数\n",
    "https://www.slideshare.net/TaroTezuka/bernoulli-distributions-and-multinomial-distributions\n",
    "\n",
    "### シグモイド関数\n",
    "\n",
    "入力値を0.0～1.0の範囲の数値に変換して出力する関数である。\n",
    "\n",
    "### スクラッチコードの完成形\n",
    "線形回帰のクラスをスクラッチで作成していきますが、最終的なコードはどのようになっているのでしょうか。\n",
    "\n",
    "下記は、最終的なコードの概観になります。\n",
    "\n",
    "```python\n",
    "class ScratchLogisticRegression():\n",
    "    def __init__(self,・・・):\n",
    "      \"\"\"\n",
    "      インスタンス変数初期化\n",
    "      \"\"\"\n",
    "      ・・・\n",
    "\n",
    "    def fit(self,・・・):\n",
    "        \"\"\"\n",
    "        線形回帰の学習\n",
    "        \"\"\"\n",
    "        ・・・\n",
    "\n",
    "\n",
    "    # 問題1\n",
    "    def _logistic_hypothesis(self,・・・):\n",
    "        \"\"\"\n",
    "        線形回帰の出力 → シグモイド関数 → 出力\n",
    "        \"\"\"\n",
    "\n",
    "    # 問題2\n",
    "    def _gradient_descent(self,・・・):\n",
    "        \"\"\"\n",
    "        最急降下法によるパラメータの更新値計算\n",
    "        \"\"\"\n",
    "        \n",
    "    # 問題3\n",
    "    def predict(self,・・・):\n",
    "        \"\"\"\n",
    "        ロジスティック回帰でのラベル推定\n",
    "        \"\"\"\n",
    "\n",
    "    # 問題3\n",
    "    def predict_proba(self,・・・):\n",
    "        \"\"\"\n",
    "        ロジスティック回帰での確率の推定\n",
    "        \"\"\"\n",
    "\n",
    "    # 問題4\n",
    "    def _loss_func(self,・・・):\n",
    "        \"\"\"\n",
    "        損失関数\n",
    "        \"\"\"\n",
    "        # 問題4\n",
    "        self._mse(・・・)\n",
    "        \n",
    "\n",
    "```\n",
    "\n",
    "## 【問題1】仮定関数\n",
    "\n",
    "https://seedata.co.jp/blog/tech/3722/\n",
    "\n",
    "### 正則化項\n",
    "\n",
    "https://ai-trend.jp/basic-study/neural-network/regularization/\n",
    "\n",
    "\n",
    "## 【問題2】最急降下法\n",
    "\n",
    "\n",
    "## 【問題3】推定\n",
    "\n",
    "ここまで作ってきた`_logistic_hypothesis`の出力は、線形回帰同様に連続値で出てきているのがわかります。\n",
    "\n",
    "`predict_proba`は、下記のような出力を想定しています。\n",
    "\n",
    "|Aが起こる確率|Aが起こらない確率|\n",
    "|--|--|\n",
    "|0.08|0.92|\n",
    "|0.15|0.85|\n",
    "|0.23|0.77|\n",
    "|0.99|0.01|\n",
    "\n",
    "`predict`は、下記のような出力を想定しています。\n",
    "|ラベル|\n",
    "|--|\n",
    "|Aが起こらない|\n",
    "|Aが起こらない|\n",
    "|Aが起こらない|\n",
    "|Aが起こらない|\n",
    "|Aが起こる|\n",
    "\n",
    "## 【問題4】目的関数\n",
    "\n",
    "線形回帰で実施したときと同様、式をひとつづつ分解し、実装していきましょう。\n",
    "\n",
    "\n",
    "## 【問題5】学習と推定\n",
    "\n",
    "ここまでの問題では、下記の関数を作成してきました。\n",
    "\n",
    "`_logistic_hypothesis`：仮定関数の出力計算\n",
    "\n",
    "`_gradient_descent`：シータの更新\n",
    "\n",
    "この問題では、この2つの関数を利用し（変更可）、冒頭で示した`ScratchLogisticRegression`を実装していくことを目的としています。ですので、まだ実装を行っていない`__init__()`と`fit()`を実装してみましょう。\n",
    "\n",
    "最急降下法の流れに則ると、下記の流れで実装していくとよいでしょう。\n",
    "1. 学習率や学習回数の初期化＆シータの初期化（`no_bias`や`verbose`については、アドバンス課題のため考慮しなくてもよい）\n",
    "2. 推定値算出\n",
    "3. 値更新\n",
    "4. 2,3を学習回数文繰り返す\n",
    "\n",
    "## 【問題6】学習曲線のプロット\n",
    "\n",
    "クラス内に描画関数を追加してもよいですし、インスタンス変数として損失値を保持しているので、クラス外部で描画してもよいです。\n",
    "\n",
    "## 【問題7】決定領域の可視化\n",
    "\n",
    "「決定領域を可視化する」とは、分類結果を分かりやすく色分けすることを言います。\n",
    "\n",
    "点の描画は問題ないかと思いますが、背景色の決定境界線の描画に関しては、使用している説明変数の範囲にある全ての値の候補に対する推定結果を描画します。ヒントとしては、numpyのmeshgridという関数を使用し、**説明変数の範囲にある全ての値の候補** を求め、predictするイメージです。\n",
    "\n",
    "https://www.dskomei.com/entry/2018/03/04/125249\n",
    "\n",
    "クラス内に描画関数を追加してもよいですし、インスタンス変数として損失値を保持しているので、クラス外部で描画してもよいです。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad5768f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

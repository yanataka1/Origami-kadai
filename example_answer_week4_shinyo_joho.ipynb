{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week4 授業課題　信用情報の学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-01T04:01:34.478809Z",
     "start_time": "2019-08-01T04:01:34.469708Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "ライブラリのインポート\n",
    "'''\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#import missingno as msno\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import roc_curve\n",
    "%matplotlib inline\n",
    "#pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題1】コンペティション内容の確認\n",
    "コンペティションのOverviewページ読み、「Home Credit Default Risk」について以下の観点について確認してください。\n",
    "\n",
    "・何を学習し、何を予測するのか<br>\n",
    "・どのようなファイルを作りKaggleに提出するか<br>\n",
    "・提出されたものはどういった指標値で評価されるのか"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ・何を学習し、何を予測するのか\n",
    "HomeCredit社が提供する個人の電話会社や取引情報を含むデータを利用し、返済能力を予測する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ・どのようなファイルを作りKaggleに提出するか\n",
    "ヘッダーを含んだ形式で、SK_ID_CURRの各サンプルについて、TARGET変数の確率を予測する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ・提出されたものはどういった指標値で評価されるのか\n",
    "\n",
    "提出物は 、予測された確率と観測されたターゲットの間のROC曲線の下の領域で評価される。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題2】学習と検証\n",
    "データを簡単に分析、前処理し、学習、検証するまでの一連の流れを作成・実行してください。\n",
    "\n",
    "検証にはこのコンペティションで使用される評価指標を用いるようにしてください。学習に用いる手法は指定しません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-01T04:01:38.615561Z",
     "start_time": "2019-08-01T04:01:34.501852Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'application_train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-b504aa8801a3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mデータセットファイルの読み込み\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m '''\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"application_train.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1048\u001b[0m             )\n\u001b[0;32m   1049\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1050\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1051\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1052\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1866\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1867\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1868\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1869\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"encoding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m         \"\"\"\n\u001b[1;32m-> 1362\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"replace\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 642\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'application_train.csv'"
     ]
    }
   ],
   "source": [
    "'''\n",
    "データセットファイルの読み込み\n",
    "'''\n",
    "df = pd.read_csv(\"application_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-01T04:01:38.645436Z",
     "start_time": "2019-08-01T04:01:38.617653Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-08-01T04:01:34.593Z"
    }
   },
   "source": [
    "# 欠損値を可視化\n",
    "msno.bar(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-08-01T04:01:34.615Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 欠損値がある行と列を削除\n",
    "df = df.dropna(how='any', axis=1)\n",
    "df = df.dropna(how='any', axis=0)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-08-01T04:01:34.726Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# target0、target1のサンプルを５００ずつランダムに取り出す（ターゲット数が不均衡のため）\n",
    "\n",
    "# target0とtarget1にデータを分割\n",
    "t0 = df.query(\"TARGET == 0\")\n",
    "t1 = df.query(\"TARGET == 1\")\n",
    "\n",
    "# ランダムに５００サンプルを抽出、データフレームに変換\n",
    "t0 = t0.sample(n=500)\n",
    "t1 = t1.sample(n=500)\n",
    "t0.head()\n",
    "t1.head()\n",
    "\n",
    "# t0とt１を結合,０と１が５００ずつのデータセットにする\n",
    "df_1000 = pd.concat([t0, t1])\n",
    "\n",
    "# X array,yarrayに格納\n",
    "y = df_1000[\"TARGET\"]\n",
    "y = np.array(y)\n",
    "X = df_1000.drop(\"TARGET\", axis=1)\n",
    "\n",
    "X = np.array(X)\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "# 訓練用データと検証用データに分割\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, train_size=0.75)\n",
    "\n",
    "# print(X_train[0])\n",
    "# print(y_train[0])\n",
    "# print(X_test[0])\n",
    "# print(y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-08-01T04:01:34.821Z"
    },
    "init_cell": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SVMで学習、推定\n",
    "svm = SVR(gamma='scale')\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "svm_pred = svm.predict(X_test)\n",
    "print(\"SVM の推定値：{}\".format(svm_pred[:4])) \n",
    "print(\"テストラベル値：{}\".format(y_test[:4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-08-01T04:01:34.846Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 評価(ROC曲線のカーブ)\n",
    "\n",
    "# FPR, TPR(, しきい値) を算出\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, svm_pred)\n",
    "\n",
    "# ついでにAUCも\n",
    "auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "# ROC曲線をプロット\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = %.2f)'%auc)\n",
    "plt.legend()\n",
    "plt.title('ROC curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題3】テストデータに対する推定\n",
    "テストデータ（application_test.csv）に対して推定を行い、Kaggleに提出を行ってください。\n",
    "\n",
    "正しく提出が行えていれば、精度は低くても構いません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-08-01T04:01:34.877Z"
    }
   },
   "outputs": [],
   "source": [
    "# テストデータを読み込む\n",
    "'''\n",
    "testデータセットファイルの読み込み\n",
    "'''\n",
    "df_test = pd.read_csv(\"application_test.csv\")\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-08-01T04:01:34.922Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-08-01T04:01:34.944Z"
    }
   },
   "outputs": [],
   "source": [
    "#カテゴリカルデータをダミー変数化\n",
    "df_test = pd.get_dummies(df_test)\n",
    "print(df_test.shape)\n",
    "# display(df_test.head())\n",
    "\n",
    "# 欠損値がある行と列を削除\n",
    "df_test = df_test.dropna(how='any', axis=1)\n",
    "df_test = df_test.dropna(how='any', axis=0)\n",
    "print(df_test.shape)\n",
    "\n",
    "#ndarrayに変換\n",
    "X_df_test = np.array(df_test)\n",
    "print(X_df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-08-01T04:01:35.020Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# テストデータを使ってSVMで推定\n",
    "svm_pred_sub = svm.predict(X_df_test)\n",
    "print(svm_pred_sub.shape)\n",
    "print(\"SVM の推定値：{}\".format(svm_pred_sub))\n",
    "\n",
    "# # 評価(ROC曲線のカーブ)\n",
    "# print(\"ROC_curve：{}\".format(roc_curve(df_test, svm_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-08-01T04:01:35.043Z"
    }
   },
   "outputs": [],
   "source": [
    "# kaggleに提出するためのCSVファイルに加工\n",
    "svm_pred_sub = pd.DataFrame(svm_pred_sub)\n",
    "svm_pred_sub = svm_pred_sub.clip(0, 0.99)\n",
    "svm_pred_sub = svm_pred_sub.rename(columns={0:\"TARGET\"})\n",
    "svm_pred_sub = pd.concat([df_test[\"SK_ID_CURR\"], svm_pred_sub], axis=1, join='inner')\n",
    "#svm_pred = svm_pred.set_index(\"SK_ID_CURR\", inplace = True)\n",
    "svm_pred_sub.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV ファイル (svm_pred.csv) として出力\n",
    "svm_pred_sub.to_csv(\"svm_pred_sub.csv\", index=False)\n",
    "\n",
    "#Kaggleに提出しました\n",
    "#Score 0.52856"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題4】特徴量エンジニアリング\n",
    "精度を上げるために以下のような観点で 特徴量エンジニアリング（Feature Engineering） を行ってください。\n",
    "\n",
    "・どの特徴量を使うか<br>\n",
    "・どう前処理をするか<br>\n",
    "何をした時に検証データに対する評価指標がどのようになったかをまとめてください。最低5パターンの学習・検証を行ってください。\n",
    "\n",
    "精度が高かったものに関してはテストデータに対しても推定を行い、Kaggleに提出を行ってください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 フィルタ法で特徴量を５０％に絞る\n",
    "### 2 標準化\n",
    "### 3 外れ値に頑健な標準化(RobustScaler)\n",
    "### 4 正規化\n",
    "### 5 OneHotEncording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-08-01T04:01:35.120Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import scipy.stats\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#カテゴリ変数加工前のデータ\n",
    "# X array,yarrayに格納\n",
    "X = df_1000.drop(\"TARGET\", axis=1)\n",
    "X = np.array(X)\n",
    "print(X.shape)\n",
    "\n",
    "y = df_1000[\"TARGET\"]\n",
    "y = np.array(y)\n",
    "print(y.shape)\n",
    "\n",
    "# 訓練用データと検証用データに分割\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, train_size=0.75)\n",
    "\n",
    "# 1 フィルタ法で特徴量を５０％に絞る\n",
    "#SelectPercentileを使って50%の特徴量を選択。分類の場合はf_classifを用いる（回帰にはf_regressionを用いる）\n",
    "select = SelectPercentile(score_func=f_regression, percentile=50)\n",
    "select.fit(X_train, y_train)\n",
    "\n",
    "#trainデータとtestデータを変換\n",
    "X_train_filter = select.transform(X_train)\n",
    "X_test_filter = select.transform(X_test)\n",
    "\n",
    "print(X_train_filter.shape)\n",
    "print(X_test_filter.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "# SVMで学習、推定\n",
    "svm = SVR(gamma='scale')\n",
    "svm.fit(X_train_filter, y_train)\n",
    "svm_pred_filter = svm.predict(X_test_filter)\n",
    "\n",
    "#2 標準化\n",
    "ss = preprocessing.StandardScaler()\n",
    "ss.fit(X)\n",
    "X_ss = ss.transform(X)\n",
    "\n",
    "# 訓練用データと検証用データに分割\n",
    "X_train_ss, X_test_ss, y_train_ss, y_test_ss = \\\n",
    "    train_test_split(X_ss, y, train_size=0.75)\n",
    "\n",
    "# SVMで学習、推定\n",
    "svm.fit(X_train_ss, y_train_ss)\n",
    "svm_pred_ss = svm.predict(X_test_ss)\n",
    "#print(\"SVM の推定値：{}\".format(svm_pred_ss[:4])) \n",
    "#print(\"テストラベル値：{}\".format(y_test_ss[:4]))\n",
    "\n",
    "\n",
    "# 3 外れ値に頑健な標準化(RobustScaler)\n",
    "rs = preprocessing.RobustScaler(quantile_range=(25., 75.))\n",
    "rs.fit(X)\n",
    "X_rs = rs.transform(X)\n",
    "\n",
    "# 訓練用データと検証用データに分割\n",
    "X_train_rs, X_test_rs, y_train_rs, y_test_rs = \\\n",
    "    train_test_split(X_rs, y, train_size=0.75)\n",
    "\n",
    "# SVMで学習、推定\n",
    "svm.fit(X_train_rs, y_train_rs)\n",
    "svm_pred_rs = svm.predict(X_test_rs)\n",
    "\n",
    "\n",
    "# 4 正規化\n",
    "mm = preprocessing.MinMaxScaler(feature_range=(0, 1), copy=True)\n",
    "mm.fit(X)\n",
    "X_mm = mm.transform(X)\n",
    "\n",
    "# 訓練用データと検証用データに分割\n",
    "X_train_mm, X_test_mm, y_train_mm, y_test_mm = \\\n",
    "    train_test_split(X_mm, y, train_size=0.75)\n",
    "\n",
    "# SVMで学習、推定\n",
    "svm.fit(X_train_mm, y_train_mm)\n",
    "svm_pred_mm = svm.predict(X_test_mm)\n",
    "\n",
    "\n",
    "# 5 OneHotEncording\n",
    "ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "X_ohe = ohe.fit_transform(X)\n",
    "\n",
    "# 訓練用データと検証用データに分割\n",
    "X_train_ohe, X_test_ohe, y_train_ohe, y_test_ohe = \\\n",
    "    train_test_split(X_ohe, y, train_size=0.75)\n",
    "\n",
    "# SVMで学習、推定\n",
    "svm.fit(X_train_ohe, y_train_ohe)\n",
    "svm_pred_ohe = svm.predict(X_test_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-08-01T04:01:35.146Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 評価(ROC曲線のカーブ)\n",
    "\n",
    "# FPR, TPR(, しきい値) を算出\n",
    "\n",
    "pred_list = [svm_pred_filter, svm_pred_ss, svm_pred_rs, svm_pred_mm, svm_pred_ohe]\n",
    "title_list = [\"1:filter\", \"2:StandardScaler\", \n",
    "              \"3:RobustScaler\",\"4:MinMaxScaler\",\"5:OneHotEncording\"]\n",
    "\n",
    "for i in range(len(pred_list)):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test, pred_list[i])\n",
    "\n",
    "    # AUC\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "    # ROC曲線をプロット\n",
    "    plt.plot(fpr, tpr, label='ROC curve'+ title_list[i] +'(area = %.2f)'%auc)\n",
    "    plt.legend()\n",
    "    plt.title('ROC curve')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.grid(True)\n",
    "\n",
    "#print(\"SVM の推定値：{}\".format(svm_pred_filter)) \n",
    "#print(\"テストラベル値：{}\".format(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## まとめ\n",
    "５つの手法で前処理や特徴量エンジニアリングを行ったが、あまり精度が上がらなかった。\n",
    "最も良いのは、1番目の特徴量のフィルタリングだった。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テストデータを使って１番の手法で特徴量を絞り、SVMで推定\n",
    "#select.fit(X_df_test)\n",
    "\n",
    "#trainデータとtestデータを変換\n",
    "#X_train_filter = select.transform(X_train)\n",
    "X_test_filter = select.transform(X_df_test)\n",
    "\n",
    "# SVMで学習、推定\n",
    "svm = SVR(gamma='scale')\n",
    "svm.fit(X_train_filter, y_train)\n",
    "svm_pred_filter = svm.predict(X_test_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kaggleに提出するためのCSVファイルに加工\n",
    "svm_pred_filter = pd.DataFrame(svm_pred_filter)\n",
    "svm_pred_sub = svm_pred_sub.abs()\n",
    "svm_pred_filter = svm_pred_filter.rename(columns={0:\"TARGET\"})\n",
    "svm_pred_filter = pd.concat([df_test[\"SK_ID_CURR\"], svm_pred_filter], axis=1, join='inner')\n",
    "#svm_pred = svm_pred.set_index(\"SK_ID_CURR\", inplace = True)\n",
    "svm_pred_filter.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV ファイル (svm_pred.csv) として出力\n",
    "svm_pred_filter.to_csv(\"svm_pred_filter.csv\", index=False)\n",
    "\n",
    "#Kaggleに提出しました"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Initialization Cell",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ec96c41",
   "metadata": {},
   "source": [
    "# 最急降下法とは\n",
    "\n",
    "## ゴール\n",
    "- **【Sprint 機械学習スクラッチ 線形回帰】の【問題2】を解くうえで必要な知識や技術について理解する**\n",
    "\n",
    "### Sprintの目的\n",
    "- 線形回帰の意味を理解する\n",
    "- 線形回帰をスクラッチ開発するのに必要な概念を理解する\n",
    "\n",
    "## どのように学ぶか\n",
    "\n",
    "【Sprint 機械学習スクラッチ 線形回帰】の目次と照らし合わせながら、進めていきましょう。\n",
    "\n",
    "## 【問題2】最急降下法\n",
    "**回帰式を解く**とは、データにもっとも当てはまりの良い、Θi(i=複数変数ある際の変数の番号)の値を求めることだと紹介しましたが、どのように、このΘを求めればいいのでしょうか。\n",
    "\n",
    "### 誤差\n",
    "\n",
    "**データにもっとも当てはまりが良い**とは、下記の図で言う`error`の値が、最も小さくなる時のことを言います。\n",
    "\n",
    "<a href=\"https://diveintocode.gyazo.com/dc4108d0eea9b579cb67ebbd8a5a7d92\"><img src=\"https://t.gyazo.com/teams/diveintocode/dc4108d0eea9b579cb67ebbd8a5a7d92.png\" alt=\"Image from Gyazo\" width=\"800\"/></a>\n",
    "\n",
    "### 平均2乗誤差\n",
    "\n",
    "引く直線によっては、`error`の値が±両方出てきますので、単純なerrorの合計値だけで、**データにもっとも当てはまりが良い**状態を判断することが出来ません。\n",
    "\n",
    "そこで、使用されるのが、平均2乗誤差と呼ばれるもので、下記の式で表されます。\n",
    "\n",
    "$$\n",
    "L(\\theta)=  \\frac{1 }{ m}  \\sum_{i=1}^{m} (h_\\theta(x^{(i)})-y^{(i)})^2.\n",
    "$$\n",
    "\n",
    "この数式の値が最小になるようなΘを求めることが、回帰式の**ゴール**となります。\n",
    "\n",
    "### 最急降下法（Θの求め方）\n",
    "\n",
    "例えば、平均2乗誤差の数式をグラフにプロットした場合、下記のようになったとします。\n",
    "\n",
    "<a href=\"https://diveintocode.gyazo.com/607d80ca9eebee99b564c1e47c946896\"><img src=\"https://t.gyazo.com/teams/diveintocode/607d80ca9eebee99b564c1e47c946896.png\" alt=\"Image from Gyazo\" width=\"800\"/></a>\n",
    "\n",
    "平均2乗誤差の値が最小になる地点は、次の位置であるとわかるかと思います。\n",
    "\n",
    "<a href=\"https://diveintocode.gyazo.com/704481108d13936ee159071457fd114c\"><img src=\"https://t.gyazo.com/teams/diveintocode/704481108d13936ee159071457fd114c.png\" alt=\"Image from Gyazo\" width=\"800\"/></a>\n",
    "\n",
    "このΘの個所をどのように特定すればいいのでしょうか。下記の手順になります。\n",
    "\n",
    "①まずは、ランダムにΘをプロットします。\n",
    "\n",
    "<a href=\"https://diveintocode.gyazo.com/2dcbcbe726bcb342cc003fccbfebf8a3\"><img src=\"https://t.gyazo.com/teams/diveintocode/2dcbcbe726bcb342cc003fccbfebf8a3.png\" alt=\"Image from Gyazo\" width=\"800\"/></a>\n",
    "\n",
    "②ランダムにプロットされたシータの位置の傾きを求める\n",
    "\n",
    "<a href=\"https://diveintocode.gyazo.com/b691334882a9eb7de2ea145e62a8e28d\"><img src=\"https://t.gyazo.com/teams/diveintocode/6b691334882a9eb7de2ea145e62a8e28d.png\" alt=\"Image from Gyazo\" width=\"800\"/></a>\n",
    "\n",
    "③傾きと学習率により、Θを更新\n",
    "\n",
    "<a href=\"https://diveintocode.gyazo.com/60b2742d1713b8c3fef99b38ec62443b\"><img src=\"https://t.gyazo.com/teams/diveintocode/60b2742d1713b8c3fef99b38ec62443b.png\" alt=\"Image from Gyazo\" width=\"800\"/></a>\n",
    "\n",
    "④①～③を繰り返す\n",
    "\n",
    "<a href=\"https://diveintocode.gyazo.com/328d2159009b54896252805af22e0f84\"><img src=\"https://t.gyazo.com/teams/diveintocode/328d2159009b54896252805af22e0f84.png\" alt=\"Image from Gyazo\" width=\"800\"/></a>\n",
    "\n",
    "**これを数式で表すと下記のようになります。**\n",
    "\n",
    "$$\n",
    "J(\\theta)=  \\frac{1 }{ 2m}  \\sum_{i=1}^{m} (h_\\theta(x^{(i)})-y^{(i)})^2.\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^{m}[(h_\\theta(x^{(i)}) - y^{(i)} )x_{j}^{(i)}]\n",
    "$$\n",
    "\n",
    "\n",
    "- ヒント：パラメータ導出のイメージ（https://diver.diveintocode.jp/questions/8028）\n",
    "- ヒント：なぜ降下法を使うのか（https://diver.diveintocode.jp/questions/8029）\n",
    "\n",
    "\n",
    "### 損失関数（目的関数）\n",
    "\n",
    "平均2乗誤差を最小にするΘを求めてきましたが、この対象となる誤差関数のことを、**損失関数（目的関数）**といいます。\n",
    "\n",
    "Θの更新の際に、平均2乗誤差を使用していると紹介しましたが、厳密には、展開後の式を分かりやすくするため、下記の数式を利用しています。\n",
    "\n",
    "$$\n",
    "L(\\theta)=  \\frac{1 }{ m}  \\sum_{i=1}^{m} (h_\\theta(x^{(i)})-y^{(i)})^2.\n",
    "$$\n",
    "\n",
    "↓\n",
    "\n",
    "$$\n",
    "J(\\theta)=  \\frac{1 }{ 2m}  \\sum_{i=1}^{m} (h_\\theta(x^{(i)})-y^{(i)})^2.\n",
    "$$\n",
    "\n",
    "どちらもΘについて微分し、係数を学習率に飲み込ませることで等価な式となります。\n",
    "\n",
    "### トイデータ\n",
    "\n",
    "作成した関数に、下記の変数を引数として与えてみましょう。\n",
    "\n",
    "まずは誤差の含まないモデルで実験してみましょう。\n",
    "例えば適当に以下のデータを利用します。\n",
    "```python\n",
    "x = np.linspace(1,6,5)\n",
    "X = np.c_[np.ones(5),x]#入力データX\n",
    "\n",
    "y = 2*x + 1#適当な真のモデル\n",
    "\n",
    "theta = [0,0]#仮定関数の係数の初期値\n",
    "y_pred = X @ theta\n",
    "\n",
    "error = y_pred - y#入力データerror\n",
    "```\n",
    "\n",
    "更新式を実装し、学習率alpha=0.05の時、更新された後のthetaが以下の様になっていれば成功です。\n",
    "\n",
    "```\n",
    "theta:[0.8, 3.1450000000000005]\n",
    "```\n",
    "以下の更新式を、行列計算で書いた場合は\n",
    "$$\n",
    "\\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^{m}[(h_\\theta(x^{(i)}) - y^{(i)} )x_{j}^{(i)}]\n",
    "$$\n",
    "\n",
    "```\n",
    "theta:[0.8, 3.425]\n",
    "```\n",
    "となります。\n",
    "\n",
    "この違いはパラメータ更新を同時に行うか、1つずつ行うかという違いになります。\n",
    "\n",
    "## まとめ\n",
    "- 最急降下法とは`y = ax1 + b`における`a`と`b`を最適な値にする手法です\n",
    "- 平均2乗誤差(損失関数)を微分することで更新式が導出されることを学びました\n",
    "- 更新過程のイメージは平均2乗誤差の山を降っていく様なイメージです"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d426eb16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

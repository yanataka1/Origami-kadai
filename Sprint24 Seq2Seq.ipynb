{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"name":"Sprint24 Seq2Seq.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"oz0g8JrH2PRE"},"source":["# 問題1　機械翻訳の実行とコードリーディング\n","\n","下記圧縮ファイルをダウンロードし、回答して、このsprint24.ipynbと同じ階層においてください。\n","\n","http://www.manythings.org/anki/fra-eng.zip\n","\n","\n","次に、下記githubにアクセスし、プログラムをコピペしてきます。\n","\n","https://github.com/rstudio/keras/blob/master/vignettes/examples/lstm_seq2seq.py\n","\n","下記は、コピペしてきたプログラムを分かりやすく分解したものになります。\n","\n","【アルゴリズム参考】\n","\n","https://blog.octopt.com/sequence-to-sequence/\n","\n","【プログラム参考】\n","\n","https://omathin.com/seq2seq-study/\n","\n","https://qiita.com/FukuharaYohei/items/27cd247342a0f7006511\n","\n","##### ダイバー\n","以下のサンプルコードは、短い英語からフランス語への変換を行うものです。これを動かしてください。\n","\n","\n","keras/lstm_seq2seq.py at master · rstudio/keras\n","\n","\n","その上でこのサンプルコードの各部分がどういった役割かを読み取り、まとめてください。以下のようにどこからどこの行が何をしているかを記述してください。\n","\n","\n","（例）\n","\n","\n","51から55行目 : ライブラリのimport\n","57から62行目 : ハイパーパラメータの設定\n","\n","《文字単位のトークン化》\n","\n","\n","この実装ではテキストのベクトル化の際に、単語ではなく文字ごとを1つのトークンとして扱っています。\n","\n","\n","scikit-learnでBoWを計算するCountVectorizerの場合では、デフォルトの引数はanalyzer=’word’で単語を扱いますが、charやchar_wbとすることで文字を扱えるようになります。\n","\n","\n","charとchar_wbの2種類の方法があり、char_wbを指定した場合、n_gramが単語内からのみ作成されます。逆にcharは単語の区切りが関係なくn_gramが作成されます。This movie is very good.というテキストを3-gramでカウントする時、charではs mやe iといった単語をまたぐ数え方もしますが、char_wbではこれらを見ません。\n","\n","\n","sklearn.feature_extraction.text.CountVectorizer — scikit-learn 0.21.3 documentation"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":70},"id":"Thn3UOkAmWTO","executionInfo":{"status":"ok","timestamp":1632316566801,"user_tz":-540,"elapsed":321,"user":{"displayName":"taka yana","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgSousXmZLErCZtm-y-NMYem1cW1DcyjGMYG8g7=s64","userId":"10868041201851028412"}},"outputId":"4db6339f-d318-416d-ac96-f477eedc6bfa"},"source":["%pwd"],"execution_count":96,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/drive/My Drive/Colab Notebooks/Sprint24 Seq2Seq/pytorch-tutorial/tutorials/03-advanced/image_captioning/pytorch-tutorial/tutorials/03-advanced/image_captioning/pytorch-tutorial/tutorials/03-advanced/image_captioning/pytorch-tutorial/tutorials/03-advanced/image_captioning/pytorch-tutorial/tutorials/03-advanced/image_captioning/pytorch-tutorial/tutorials/03-advanced/image_captioning/pytorch-tutorial/tutorials/03-advanced/image_captioning'"]},"metadata":{},"execution_count":96}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"THKNYEmd3Qcg","executionInfo":{"status":"ok","timestamp":1632315244076,"user_tz":-540,"elapsed":375,"user":{"displayName":"taka yana","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgSousXmZLErCZtm-y-NMYem1cW1DcyjGMYG8g7=s64","userId":"10868041201851028412"}},"outputId":"77263de5-c441-4a66-c0c2-6b256e84d770"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hwBRlTqp3af-","executionInfo":{"status":"ok","timestamp":1632316415133,"user_tz":-540,"elapsed":550,"user":{"displayName":"taka yana","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgSousXmZLErCZtm-y-NMYem1cW1DcyjGMYG8g7=s64","userId":"10868041201851028412"}},"outputId":"bb4e173e-531f-478c-b193-f6b2e7eb81d6"},"source":["%ls"],"execution_count":95,"outputs":[{"output_type":"stream","name":"stdout","text":["build_vocab.py  model.py      README.md         sample.py\n","data_loader.py  \u001b[0m\u001b[01;34mpng\u001b[0m/          requirements.txt  train.py\n","download.sh     \u001b[01;34m__pycache__\u001b[0m/  resize.py\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tCtMRmH73chj","executionInfo":{"status":"ok","timestamp":1632315261428,"user_tz":-540,"elapsed":6,"user":{"displayName":"taka yana","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgSousXmZLErCZtm-y-NMYem1cW1DcyjGMYG8g7=s64","userId":"10868041201851028412"}},"outputId":"a2485ad7-7aca-47ce-b17d-9054e585744d"},"source":["%cd Sprint24 Seq2Seq"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[Errno 2] No such file or directory: 'Sprint24 Seq2Seq'\n","/content/drive/My Drive/Colab Notebooks/Sprint24 Seq2Seq/pytorch-tutorial/tutorials/03-advanced/image_captioning/pytorch-tutorial/tutorials/03-advanced/image_captioning/pytorch-tutorial/tutorials/03-advanced/image_captioning/pytorch-tutorial/tutorials/03-advanced/image_captioning/pytorch-tutorial/tutorials/03-advanced/image_captioning/pytorch-tutorial/tutorials/03-advanced/image_captioning/pytorch-tutorial/tutorials/03-advanced/image_captioning\n"]}]},{"cell_type":"code","metadata":{"id":"4TE95m8Y2PRP"},"source":["# ライブラリのimport\n","from __future__ import print_function\n","from keras.models import Model\n","from keras.layers import Input, LSTM, Dense\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K6yO7GuSl2lF"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0B9tKiFU2PRS"},"source":["# 必要な変数の定義\n","## 学習関係\n","batch_size = 1  #64 バッチサイズ.\n","epochs = 1  #10 学習回数\n","latent_dim = 256  # \n","num_samples = 10000  # サンプル数\n","\n","## ファイル関係\n","data_path = 'fra-eng/fra.txt' # データパス\n","\n","## データ関係\n","input_texts = [] # 入力テキスト（説明変数）\n","target_texts = [] # 翻訳テキスト（目的変数）\n","input_characters = set() # 一意な入力文字\n","target_characters = set() # 一意な翻訳文字"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nvBjl_vx2PRU","colab":{"base_uri":"https://localhost:8080/","height":240},"executionInfo":{"status":"error","timestamp":1632314866923,"user_tz":-540,"elapsed":302,"user":{"displayName":"taka yana","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgSousXmZLErCZtm-y-NMYem1cW1DcyjGMYG8g7=s64","userId":"10868041201851028412"}},"outputId":"ce7d5f48-ab25-4d67-984d-b74602c6e53c"},"source":["# データの読み込み\n","with open(data_path, 'r', encoding='utf-8') as f:\n","    lines = f.read().split('\\n')\n","\n","# 取得した行でループ\n","for line in lines[: min(num_samples, len(lines) - 1)]:\n","    # タブで区切られている説明変数と目的変数を分離\n","    input_text, target_text, _ = line.split('\\t')\n","    # タブと改行で挟む\n","    target_text = '\\t' + target_text + '\\n'\n","    # 説明変数と目的変数をそれぞれ追加\n","    input_texts.append(input_text)\n","    target_texts.append(target_text)\n","    # 一意な文字それぞれ保存\n","    for char in input_text:\n","        if char not in input_characters:\n","            input_characters.add(char)\n","    for char in target_text:\n","        if char not in target_characters:\n","            target_characters.add(char)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-81-13b492057e03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# データの読み込み\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# 取得した行でループ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'fra-eng/fra.txt'"]}]},{"cell_type":"code","metadata":{"id":"6Y9tZvQd2PRW"},"source":["# 学習推定に必要な変数の定義\n","input_characters = sorted(list(input_characters))\n","target_characters = sorted(list(target_characters))\n","num_encoder_tokens = len(input_characters)\n","num_decoder_tokens = len(target_characters)\n","max_encoder_seq_length = max([len(txt) for txt in input_texts])\n","max_decoder_seq_length = max([len(txt) for txt in target_texts])\n","\n","print('Number of samples:', len(input_texts))\n","print('Number of unique input tokens:', num_encoder_tokens)\n","print('Number of unique output tokens:', num_decoder_tokens)\n","print('Max sequence length for inputs:', max_encoder_seq_length)\n","print('Max sequence length for outputs:', max_decoder_seq_length)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P5cXZZvc2PRX"},"source":["# 一意な文字の辞書化\n","input_token_index = dict(\n","    [(char, i) for i, char in enumerate(input_characters)])\n","target_token_index = dict(\n","    [(char, i) for i, char in enumerate(target_characters)])\n","\n","# 入力サンプル数　×　最大文字数　×　一意な文字数（エンコーダ：特徴抽出）\n","encoder_input_data = np.zeros(\n","    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n","    dtype='float32')\n","# 入力サンプル数　×　最大文字数　×　一意な文字数（デコーダ：翻訳）\n","decoder_input_data = np.zeros(\n","    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n","    dtype='float32')\n","# 入力サンプル数　×　最大文字数　×　一意な文字数（デコーダ：翻訳）\n","decoder_target_data = np.zeros(\n","    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n","    dtype='float32')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8AXBHHK42PRZ"},"source":["# 初期化していたencoder_input_data/decoder_input_data/decoder_target_dataの該当箇所に1を代入（one-hot）\n","for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n","    for t, char in enumerate(input_text):\n","        encoder_input_data[i, t, input_token_index[char]] = 1.\n","    encoder_input_data[i, t + 1:, input_token_index[' ']] = 1.\n","    for t, char in enumerate(target_text):\n","        decoder_input_data[i, t, target_token_index[char]] = 1.\n","        if t > 0:\n","            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n","    decoder_input_data[i, t + 1:, target_token_index[' ']] = 1.\n","    decoder_target_data[i, t:, target_token_index[' ']] = 1."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NNEHkFTB2PRc"},"source":["# 入力データ数は未定・入力特徴量数はnum_encoder_tokens（一意な文字数）\n","encoder_inputs = Input(shape=(None, num_encoder_tokens))\n","# LSTM（エンコーダ）\n","encoder = LSTM(latent_dim, return_state=True)\n","# 中間状態取得\n","encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n","encoder_states = [state_h, state_c]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CSYdcYSh2PRd"},"source":["# 入力データ数は未定・入力特徴量数はnum_decoder_tokens（一意な文字数）\n","decoder_inputs = Input(shape=(None, num_decoder_tokens))\n","# LSTM（デコーダ）\n","decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n","decoder_outputs, _, _ = decoder_lstm(decoder_inputs,initial_state=encoder_states)\n","# 通常層に入れる\n","decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n","decoder_outputs = decoder_dense(decoder_outputs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aLVWG9oT2PRf"},"source":["# モデル結合\n","model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n","\n","# コンパイル\n","model.compile(optimizer='rmsprop', loss='categorical_crossentropy',metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4LcqAHVr2PRh"},"source":["# 学習と保存\n","model.fit(\n","    [encoder_input_data, decoder_input_data],\n","    decoder_target_data,\n","    batch_size=batch_size,\n","    epochs=epochs,\n","    validation_split=0.2\n",")\n","\n","model.save('s2s.h5')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G_ms8WB52PRh"},"source":["# エンコーダ\n","encoder_model = Model(encoder_inputs, encoder_states)\n","\n","# デコーダ\n","decoder_state_input_h = Input(shape=(latent_dim,))\n","decoder_state_input_c = Input(shape=(latent_dim,))\n","decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n","decoder_outputs, state_h, state_c = decoder_lstm(\n","    decoder_inputs, \n","    initial_state=decoder_states_inputs\n",")\n","\n","decoder_states = [state_h, state_c]\n","decoder_outputs = decoder_dense(decoder_outputs)\n","decoder_model = Model(\n","    [decoder_inputs] + decoder_states_inputs,\n","    [decoder_outputs] + decoder_states\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qoCuWy0h2PRj"},"source":["# 予測から文字列に起こすための辞書\n","reverse_input_char_index = dict(\n","    (i, char) for char, i in input_token_index.items())\n","reverse_target_char_index = dict(\n","    (i, char) for char, i in target_token_index.items())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U3IFI-bP2PRj"},"source":["def decode_sequence(input_seq):\n","    # 入力ベクトルをエンコーダへ\n","    states_value = encoder_model.predict(input_seq)\n","\n","    # 0で初期化\n","    target_seq = np.zeros((1, 1, num_decoder_tokens))\n","    # 先頭にタブ\n","    target_seq[0, 0, target_token_index['\\t']] = 1.\n","\n","    # 条件を満たすまでループ\n","    stop_condition = False\n","    # 最終的な文章\n","    decoded_sentence = ''\n","    while not stop_condition:\n","        # 予測\n","        output_tokens, h, c = decoder_model.predict(\n","            [target_seq] + states_value)\n","\n","        # 予測値を確定させ、数値から文字列に復元させる\n","        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n","        sampled_char = reverse_target_char_index[sampled_token_index]\n","        decoded_sentence += sampled_char\n","\n","        # 最大文字数に達するか改行文字\n","        if (sampled_char == '\\n' or\n","           len(decoded_sentence) > max_decoder_seq_length):\n","            stop_condition = True\n","\n","        # デコーダへの入力をupdate\n","        target_seq = np.zeros((1, 1, num_decoder_tokens))\n","        target_seq[0, 0, sampled_token_index] = 1.\n","        states_value = [h, c]\n","\n","    return decoded_sentence"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9uVqa_C52PRk"},"source":["# 100文章分ループ\n","for seq_index in range(10): #100\n","    input_seq = encoder_input_data[seq_index: seq_index + 1]\n","    decoded_sentence = decode_sequence(input_seq)\n","    print('-')\n","    print('Input sentence:', input_texts[seq_index])\n","    print('Decoded sentence:', decoded_sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TJF_qb9k2PRl"},"source":["# 問題2　イメージキャプショニングの学習済みモデルの実行\n","\n","他の活用例としてイメージキャプショニングがあります。画像に対する説明の文章を推定するタスクです。これは画像を入力し、系列データを出力する Image to Sequence の手法によって行えます。\n","\n","\n","pytorch-tutorial/tutorials/03-advanced/image_captioning at master · yunjey/pytorch-tutorial\n","\n","\n","イメージキャプショニングは学習に多くの時間がかかるため、ここでは学習済みの重みが公開されている実装を動かすことにします。Kerasには平易に扱える実装が公開されていないため、今回はPyTorchによる実装を扱います。\n","\n","\n","\n","上記実装において 5. Test the model の項目を実行してください。また、自身で用意した画像に対しても文章を生成してください。これらに対してどういった文章が出力されたかを記録して提出してください。\n","\n","\n","データセットからの学習は行わず、学習済みの重みをダウンロードして利用します。\n","\n","\n","注意点として、デフォルトで設定されている重みのファイル名と、ダウンロードできる重みのファイル名は異なっています。ここは書き換える必要があります。"]},{"cell_type":"code","metadata":{"id":"gytVdw7U2PRm"},"source":["# レポジトリからチュートリアルプログラム取得\n","!git clone https://github.com/yunjey/pytorch-tutorial.git"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CbkEAOma2PRm"},"source":["# 実行したいpyファイルのある場所に移動\n","%cd pytorch-tutorial/tutorials/03-advanced/image_captioning/"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3LBMV6962PRn"},"source":["# 必要モジュールのインストール\n","!pip install -r requirements.txt\n","!pip install torchvision \n","!pip install pycocotools"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ge28W_6k2PRn"},"source":["# ライブラリimport\n","import torch\n","import torchvision\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4LOeuATJ2PRp"},"source":["# 試しに出力\n","from IPython.display import Image,display_png\n","import pickle\n","\n","\n","with open('serialized.pkl', 'rb') as f:\n","    data = pickle.load(f)display_png(Image('png/example.png'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ncHaUYmb2PRp"},"source":["\n","サンプルプログラム実行のためには、下記のURLの記載に従って、学習済みモデルをダウンロードしてくる必要がある\n","\n","https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/03-advanced/image_captioning#pretrained-model\n","\n","そしてそれぞれのファイルを、下記のようにディレクトリに配置する。\n","\n","pytorch-tutorial/tutorials/03-advanced/image_captioning/data/vocab.pkl\n","\n","pytorch-tutorial/tutorials/03-advanced/image_captioning/models/decoder-5-3000.pkl\n","\n","pytorch-tutorial/tutorials/03-advanced/image_captioning/models/encoder-5-3000.pkl"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"a6nYiMHA2PRq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632315797248,"user_tz":-540,"elapsed":2299,"user":{"displayName":"taka yana","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgSousXmZLErCZtm-y-NMYem1cW1DcyjGMYG8g7=s64","userId":"10868041201851028412"}},"outputId":"ae660d4d-94c4-4f3a-e30f-3089570d060d"},"source":["# サンプルプログラム実行\n","!python sample.py --image=\"png/example.png\""],"execution_count":94,"outputs":[{"output_type":"stream","name":"stdout","text":["Traceback (most recent call last):\n","  File \"sample.py\", line 81, in <module>\n","    main(args)\n","  File \"sample.py\", line 33, in main\n","    with open(args.vocab_path, 'rb') as f:\n","FileNotFoundError: [Errno 2] No such file or directory: 'data/vocab.pkl'\n"]}]},{"cell_type":"markdown","metadata":{"id":"XZTNi4rX2PRq"},"source":["# 問題3　Kerasで動かしたい場合はどうするかを調査\n","\n","PyTorchによる実装を動かしましたが、何らかの理由からKerasで動かしたい状況が考えられます。どういった手順を踏むことになるか調査し、できるだけ詳しく説明してください。\n","\n","\n","特に今回はPyTorchのための学習済みの重みをKerasで使えるようにしたいので、その点については必ず触れてください。"]},{"cell_type":"markdown","metadata":{"id":"KaBiLsGg2PRr"},"source":["下記のようなOSSのツールを利用する\n","\n","https://github.com/gmalivenko/pytorch2keras\n","\n","https://github.com/Microsoft/MMdnn"]}]}
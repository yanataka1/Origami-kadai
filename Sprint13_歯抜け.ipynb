{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint 13　TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ライブラリのimport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "%matplotlib inline\n",
    "# tensorflow1系\n",
    "# import tensorflow as tf\n",
    "# tensorflow2系\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 読み込み\n",
    "iris_dataset = load_iris()\n",
    "\n",
    "# 整形\n",
    "iris_dataframe = pd.DataFrame(data=iris_dataset.data, columns=iris_dataset.feature_names)\n",
    "iris_datalabel = pd.DataFrame(data=iris_dataset.target,columns=['Species'])\n",
    "df = pd.concat([iris_dataframe,iris_datalabel],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ミニバッチクラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetMiniBatch:\n",
    "\n",
    "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self.X[p0:p1], self.y[p0:p1]        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self.X[p0:p1], self.y[p0:p1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 問題2　スクラッチとTensorFlowの対応を考える"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データ準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2値分類のため絞り込み\n",
    "df2 = df[(df[\"Species\"] == 0)|(df[\"Species\"] == 1)]\n",
    "\n",
    "# 説明変数と目的変数に分割\n",
    "y = df2[\"Species\"]\n",
    "X = df2.loc[:, [\"sepal length (cm)\", \"sepal width (cm)\", \"petal length (cm)\", \"petal width (cm)\"]]\n",
    "y = np.array(y)\n",
    "X = np.array(X)\n",
    "y = y.astype(np.int)[:, np.newaxis]\n",
    "\n",
    "# 訓練データ/テストデータ/評価データに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "# 正規化\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "X_val /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensorflowで学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train_loss : 0.9164, val_loss : 0.6707, train_acc : 0.469, val_acc : 0.625\n",
      "Epoch 1, train_loss : 0.7579, val_loss : 0.9342, train_acc : 0.531, val_acc : 0.375\n",
      "Epoch 2, train_loss : 0.6257, val_loss : 0.7081, train_acc : 0.531, val_acc : 0.375\n",
      "Epoch 3, train_loss : 1.0717, val_loss : 0.7457, train_acc : 0.469, val_acc : 0.625\n",
      "Epoch 4, train_loss : 1.3296, val_loss : 0.9096, train_acc : 0.469, val_acc : 0.625\n",
      "Epoch 5, train_loss : 0.7333, val_loss : 0.5380, train_acc : 0.469, val_acc : 0.625\n",
      "Epoch 6, train_loss : 0.6712, val_loss : 0.4995, train_acc : 0.469, val_acc : 0.625\n",
      "Epoch 7, train_loss : 0.8281, val_loss : 0.5774, train_acc : 0.469, val_acc : 0.625\n",
      "Epoch 8, train_loss : 0.9483, val_loss : 0.6468, train_acc : 0.469, val_acc : 0.625\n",
      "Epoch 9, train_loss : 0.8210, val_loss : 0.5635, train_acc : 0.469, val_acc : 0.625\n",
      "Epoch 10, train_loss : 0.7163, val_loss : 0.4966, train_acc : 0.469, val_acc : 0.625\n",
      "Epoch 11, train_loss : 0.6998, val_loss : 0.4820, train_acc : 0.469, val_acc : 0.625\n",
      "Epoch 12, train_loss : 0.7156, val_loss : 0.4868, train_acc : 0.469, val_acc : 0.625\n",
      "Epoch 13, train_loss : 0.7049, val_loss : 0.4761, train_acc : 0.469, val_acc : 0.625\n",
      "Epoch 14, train_loss : 0.6648, val_loss : 0.4480, train_acc : 0.469, val_acc : 0.625\n",
      "Epoch 15, train_loss : 0.6242, val_loss : 0.4199, train_acc : 0.469, val_acc : 0.625\n",
      "Epoch 16, train_loss : 0.5956, val_loss : 0.3990, train_acc : 0.500, val_acc : 0.688\n",
      "Epoch 17, train_loss : 0.5743, val_loss : 0.3826, train_acc : 0.500, val_acc : 0.688\n",
      "Epoch 18, train_loss : 0.5527, val_loss : 0.3661, train_acc : 0.531, val_acc : 0.688\n",
      "Epoch 19, train_loss : 0.5274, val_loss : 0.3476, train_acc : 0.578, val_acc : 0.750\n",
      "Epoch 20, train_loss : 0.4998, val_loss : 0.3280, train_acc : 0.641, val_acc : 0.812\n",
      "Epoch 21, train_loss : 0.4716, val_loss : 0.3083, train_acc : 0.672, val_acc : 0.875\n",
      "Epoch 22, train_loss : 0.4441, val_loss : 0.2892, train_acc : 0.688, val_acc : 0.875\n",
      "Epoch 23, train_loss : 0.4172, val_loss : 0.2707, train_acc : 0.750, val_acc : 0.875\n",
      "Epoch 24, train_loss : 0.3907, val_loss : 0.2526, train_acc : 0.797, val_acc : 0.938\n",
      "Epoch 25, train_loss : 0.3639, val_loss : 0.2348, train_acc : 0.844, val_acc : 0.938\n",
      "Epoch 26, train_loss : 0.3376, val_loss : 0.2176, train_acc : 0.859, val_acc : 0.938\n",
      "Epoch 27, train_loss : 0.3118, val_loss : 0.2012, train_acc : 0.906, val_acc : 0.938\n",
      "Epoch 28, train_loss : 0.2873, val_loss : 0.1858, train_acc : 0.922, val_acc : 1.000\n",
      "Epoch 29, train_loss : 0.2645, val_loss : 0.1719, train_acc : 0.922, val_acc : 1.000\n",
      "Epoch 30, train_loss : 0.2441, val_loss : 0.1594, train_acc : 0.938, val_acc : 1.000\n",
      "Epoch 31, train_loss : 0.2260, val_loss : 0.1485, train_acc : 0.953, val_acc : 1.000\n",
      "Epoch 32, train_loss : 0.2101, val_loss : 0.1388, train_acc : 0.969, val_acc : 1.000\n",
      "Epoch 33, train_loss : 0.1962, val_loss : 0.1303, train_acc : 0.969, val_acc : 1.000\n",
      "Epoch 34, train_loss : 0.1837, val_loss : 0.1227, train_acc : 0.984, val_acc : 1.000\n",
      "Epoch 35, train_loss : 0.1726, val_loss : 0.1158, train_acc : 0.984, val_acc : 1.000\n",
      "Epoch 36, train_loss : 0.1625, val_loss : 0.1096, train_acc : 0.984, val_acc : 1.000\n",
      "Epoch 37, train_loss : 0.1533, val_loss : 0.1039, train_acc : 0.984, val_acc : 1.000\n",
      "Epoch 38, train_loss : 0.1450, val_loss : 0.0986, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 39, train_loss : 0.1374, val_loss : 0.0938, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 40, train_loss : 0.1304, val_loss : 0.0893, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 41, train_loss : 0.1239, val_loss : 0.0852, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 42, train_loss : 0.1180, val_loss : 0.0812, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 43, train_loss : 0.1125, val_loss : 0.0775, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 44, train_loss : 0.1072, val_loss : 0.0741, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 45, train_loss : 0.1025, val_loss : 0.0709, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 46, train_loss : 0.0980, val_loss : 0.0679, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 47, train_loss : 0.0939, val_loss : 0.0650, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 48, train_loss : 0.0898, val_loss : 0.0623, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 49, train_loss : 0.0862, val_loss : 0.0597, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 50, train_loss : 0.0826, val_loss : 0.0572, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 51, train_loss : 0.0793, val_loss : 0.0549, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 52, train_loss : 0.0762, val_loss : 0.0527, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 53, train_loss : 0.0733, val_loss : 0.0506, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 54, train_loss : 0.0705, val_loss : 0.0487, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 55, train_loss : 0.0678, val_loss : 0.0468, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 56, train_loss : 0.0653, val_loss : 0.0449, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 57, train_loss : 0.0630, val_loss : 0.0432, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 58, train_loss : 0.0607, val_loss : 0.0416, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 59, train_loss : 0.0585, val_loss : 0.0401, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 60, train_loss : 0.0565, val_loss : 0.0385, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 61, train_loss : 0.0546, val_loss : 0.0372, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 62, train_loss : 0.0527, val_loss : 0.0358, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 63, train_loss : 0.0510, val_loss : 0.0345, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 64, train_loss : 0.0493, val_loss : 0.0333, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 65, train_loss : 0.0476, val_loss : 0.0321, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 66, train_loss : 0.0461, val_loss : 0.0310, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 67, train_loss : 0.0447, val_loss : 0.0299, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 68, train_loss : 0.0432, val_loss : 0.0290, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 69, train_loss : 0.0420, val_loss : 0.0278, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 70, train_loss : 0.0406, val_loss : 0.0270, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 71, train_loss : 0.0394, val_loss : 0.0261, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 72, train_loss : 0.0383, val_loss : 0.0252, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 73, train_loss : 0.0371, val_loss : 0.0245, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 74, train_loss : 0.0360, val_loss : 0.0236, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 75, train_loss : 0.0350, val_loss : 0.0229, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 76, train_loss : 0.0340, val_loss : 0.0221, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 77, train_loss : 0.0330, val_loss : 0.0215, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 78, train_loss : 0.0321, val_loss : 0.0207, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 79, train_loss : 0.0312, val_loss : 0.0201, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 80, train_loss : 0.0304, val_loss : 0.0195, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 81, train_loss : 0.0296, val_loss : 0.0189, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 82, train_loss : 0.0288, val_loss : 0.0183, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 83, train_loss : 0.0280, val_loss : 0.0178, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 84, train_loss : 0.0273, val_loss : 0.0173, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 85, train_loss : 0.0266, val_loss : 0.0168, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 86, train_loss : 0.0259, val_loss : 0.0163, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 87, train_loss : 0.0253, val_loss : 0.0158, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 88, train_loss : 0.0246, val_loss : 0.0154, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 89, train_loss : 0.0240, val_loss : 0.0149, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 90, train_loss : 0.0234, val_loss : 0.0146, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 91, train_loss : 0.0229, val_loss : 0.0141, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 92, train_loss : 0.0223, val_loss : 0.0138, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 93, train_loss : 0.0218, val_loss : 0.0133, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 94, train_loss : 0.0213, val_loss : 0.0131, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 95, train_loss : 0.0208, val_loss : 0.0126, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 96, train_loss : 0.0203, val_loss : 0.0124, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 97, train_loss : 0.0199, val_loss : 0.0120, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 98, train_loss : 0.0194, val_loss : 0.0117, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 99, train_loss : 0.0190, val_loss : 0.0114, train_acc : 1.000, val_acc : 1.000\n",
      "test_acc : 1.000\n"
     ]
    }
   ],
   "source": [
    "# 各種変数定義\n",
    "learning_rate = 0.01\n",
    "batch_size = 10\n",
    "num_epochs = 100\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = XXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
    "\n",
    "# 空配列定義\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# ミニバッチイテレータ生成\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
    "\n",
    "def example_net(x):\n",
    "    \"\"\"tensorflowを利用したニューラルネットワーク\n",
    "    Parameters\n",
    "    ---------------\n",
    "    x : 入力配列\n",
    "    \"\"\"\n",
    "    # 重み定義\n",
    "    weights = {\n",
    "        'w1': XXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    # バイアス定義\n",
    "    biases = {\n",
    "        'b1': XXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "    # 計算グラフ構築（順伝播処理）\n",
    "    layer_1 = XXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
    "    layer_1 = XXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3']\n",
    "    return layer_output\n",
    "\n",
    "# 計算グラフ受け取る\n",
    "logits = example_net(X)\n",
    "# 損失定義\n",
    "loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n",
    "# 最適化手法の定義\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "# 最適化手法で、定義した損失を最小化するルールを作成\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "# ACC計算\n",
    "correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# 変数を扱うためのおまじない\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# tensorflowのセッション開始\n",
    "with tf.Session() as sess:\n",
    "    # 初期化\n",
    "    sess.run(init)\n",
    "    # 学習回数分ループ\n",
    "    for epoch in range(num_epochs):\n",
    "        # ミニバッチイテレータでループ\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            # 「最適化手法で、定義した損失を最小化するルール」を実行\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "        # 記録\n",
    "        train_loss, train_acc = XXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
    "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
    "        # 仮定出力\n",
    "        print(\"Epoch {}, train_loss : {:.4f}, val_loss : {:.4f}, train_acc : {:.3f}, val_acc : {:.3f}\".format(epoch, train_loss, val_loss, train_acc, val_acc))\n",
    "    \n",
    "    # 学習が終了したらテストデータで実行\n",
    "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
    "    #prediction = sess.run(logits, feed_dict={X: X_test, Y: y_test})\n",
    "    #print(prediction)\n",
    "    print(\"test_acc : {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 問題3　3種類すべての目的変数を使用したIrisのモデルを作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データ準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 説明変数と目的変数に分割\n",
    "y = df[\"Species\"]\n",
    "X = df.loc[:, [\"sepal length (cm)\", \"sepal width (cm)\", \"petal length (cm)\", \"petal width (cm)\"]]\n",
    "y = np.array(y)\n",
    "X = np.array(X)\n",
    "y = y.astype(np.int)[:, np.newaxis]\n",
    "\n",
    "# 訓練データ/テストデータ/評価データに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "# onehotベクトル化\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train)\n",
    "y_val_one_hot = enc.transform(y_val)\n",
    "y_test_one_hot = enc.transform(y_test)\n",
    "\n",
    "# 正規化\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "X_val /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensorflowで学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train_loss : 12.2412, val_loss : 11.8584, train_acc : 0.323, val_acc : 0.333\n",
      "Epoch 1, train_loss : 5.3572, val_loss : 5.5115, train_acc : 0.312, val_acc : 0.292\n",
      "Epoch 2, train_loss : 1.6701, val_loss : 1.6385, train_acc : 0.365, val_acc : 0.375\n",
      "Epoch 3, train_loss : 2.5035, val_loss : 2.4677, train_acc : 0.323, val_acc : 0.333\n",
      "Epoch 4, train_loss : 1.9392, val_loss : 1.9181, train_acc : 0.323, val_acc : 0.333\n",
      "Epoch 5, train_loss : 1.7058, val_loss : 1.6793, train_acc : 0.323, val_acc : 0.333\n",
      "Epoch 6, train_loss : 1.5436, val_loss : 1.5069, train_acc : 0.323, val_acc : 0.333\n",
      "Epoch 7, train_loss : 1.5682, val_loss : 1.5244, train_acc : 0.323, val_acc : 0.333\n",
      "Epoch 8, train_loss : 1.6142, val_loss : 1.5692, train_acc : 0.323, val_acc : 0.333\n",
      "Epoch 9, train_loss : 1.6300, val_loss : 1.5845, train_acc : 0.323, val_acc : 0.333\n",
      "Epoch 10, train_loss : 1.5732, val_loss : 1.5254, train_acc : 0.323, val_acc : 0.333\n",
      "Epoch 11, train_loss : 1.4651, val_loss : 1.4155, train_acc : 0.323, val_acc : 0.333\n",
      "Epoch 12, train_loss : 1.3543, val_loss : 1.3048, train_acc : 0.323, val_acc : 0.333\n",
      "Epoch 13, train_loss : 1.2610, val_loss : 1.2125, train_acc : 0.323, val_acc : 0.333\n",
      "Epoch 14, train_loss : 1.1729, val_loss : 1.1249, train_acc : 0.323, val_acc : 0.333\n",
      "Epoch 15, train_loss : 1.0801, val_loss : 1.0332, train_acc : 0.344, val_acc : 0.333\n",
      "Epoch 16, train_loss : 0.9864, val_loss : 0.9420, train_acc : 0.375, val_acc : 0.375\n",
      "Epoch 17, train_loss : 0.8979, val_loss : 0.8575, train_acc : 0.396, val_acc : 0.458\n",
      "Epoch 18, train_loss : 0.8207, val_loss : 0.7851, train_acc : 0.417, val_acc : 0.542\n",
      "Epoch 19, train_loss : 0.7603, val_loss : 0.7294, train_acc : 0.573, val_acc : 0.750\n",
      "Epoch 20, train_loss : 0.7162, val_loss : 0.6891, train_acc : 0.688, val_acc : 0.750\n",
      "Epoch 21, train_loss : 0.6835, val_loss : 0.6590, train_acc : 0.719, val_acc : 0.750\n",
      "Epoch 22, train_loss : 0.6573, val_loss : 0.6343, train_acc : 0.729, val_acc : 0.750\n",
      "Epoch 23, train_loss : 0.6347, val_loss : 0.6125, train_acc : 0.740, val_acc : 0.750\n",
      "Epoch 24, train_loss : 0.6139, val_loss : 0.5919, train_acc : 0.740, val_acc : 0.750\n",
      "Epoch 25, train_loss : 0.5939, val_loss : 0.5719, train_acc : 0.760, val_acc : 0.750\n",
      "Epoch 26, train_loss : 0.5747, val_loss : 0.5526, train_acc : 0.771, val_acc : 0.792\n",
      "Epoch 27, train_loss : 0.5562, val_loss : 0.5340, train_acc : 0.781, val_acc : 0.833\n",
      "Epoch 28, train_loss : 0.5383, val_loss : 0.5161, train_acc : 0.812, val_acc : 0.833\n",
      "Epoch 29, train_loss : 0.5214, val_loss : 0.4994, train_acc : 0.833, val_acc : 0.875\n",
      "Epoch 30, train_loss : 0.5053, val_loss : 0.4836, train_acc : 0.844, val_acc : 0.875\n",
      "Epoch 31, train_loss : 0.4901, val_loss : 0.4690, train_acc : 0.885, val_acc : 0.875\n",
      "Epoch 32, train_loss : 0.4756, val_loss : 0.4554, train_acc : 0.896, val_acc : 0.875\n",
      "Epoch 33, train_loss : 0.4620, val_loss : 0.4428, train_acc : 0.906, val_acc : 0.875\n",
      "Epoch 34, train_loss : 0.4490, val_loss : 0.4313, train_acc : 0.906, val_acc : 0.917\n",
      "Epoch 35, train_loss : 0.4366, val_loss : 0.4205, train_acc : 0.906, val_acc : 0.917\n",
      "Epoch 36, train_loss : 0.4249, val_loss : 0.4106, train_acc : 0.896, val_acc : 0.917\n",
      "Epoch 37, train_loss : 0.4137, val_loss : 0.4015, train_acc : 0.896, val_acc : 0.917\n",
      "Epoch 38, train_loss : 0.4031, val_loss : 0.3930, train_acc : 0.906, val_acc : 0.917\n",
      "Epoch 39, train_loss : 0.3931, val_loss : 0.3852, train_acc : 0.917, val_acc : 0.917\n",
      "Epoch 40, train_loss : 0.3837, val_loss : 0.3778, train_acc : 0.927, val_acc : 0.917\n",
      "Epoch 41, train_loss : 0.3748, val_loss : 0.3709, train_acc : 0.938, val_acc : 0.917\n",
      "Epoch 42, train_loss : 0.3663, val_loss : 0.3643, train_acc : 0.938, val_acc : 0.875\n",
      "Epoch 43, train_loss : 0.3581, val_loss : 0.3579, train_acc : 0.948, val_acc : 0.917\n",
      "Epoch 44, train_loss : 0.3502, val_loss : 0.3518, train_acc : 0.948, val_acc : 0.917\n",
      "Epoch 45, train_loss : 0.3425, val_loss : 0.3458, train_acc : 0.948, val_acc : 0.917\n",
      "Epoch 46, train_loss : 0.3350, val_loss : 0.3400, train_acc : 0.948, val_acc : 0.917\n",
      "Epoch 47, train_loss : 0.3276, val_loss : 0.3343, train_acc : 0.948, val_acc : 0.917\n",
      "Epoch 48, train_loss : 0.3204, val_loss : 0.3288, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 49, train_loss : 0.3134, val_loss : 0.3234, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 50, train_loss : 0.3065, val_loss : 0.3182, train_acc : 0.969, val_acc : 0.917\n",
      "Epoch 51, train_loss : 0.2997, val_loss : 0.3132, train_acc : 0.969, val_acc : 0.917\n",
      "Epoch 52, train_loss : 0.2932, val_loss : 0.3083, train_acc : 0.969, val_acc : 0.917\n",
      "Epoch 53, train_loss : 0.2867, val_loss : 0.3035, train_acc : 0.969, val_acc : 0.917\n",
      "Epoch 54, train_loss : 0.2805, val_loss : 0.2989, train_acc : 0.969, val_acc : 0.917\n",
      "Epoch 55, train_loss : 0.2744, val_loss : 0.2945, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 56, train_loss : 0.2684, val_loss : 0.2901, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 57, train_loss : 0.2626, val_loss : 0.2860, train_acc : 0.958, val_acc : 0.958\n",
      "Epoch 58, train_loss : 0.2570, val_loss : 0.2820, train_acc : 0.958, val_acc : 0.958\n",
      "Epoch 59, train_loss : 0.2515, val_loss : 0.2781, train_acc : 0.958, val_acc : 0.958\n",
      "Epoch 60, train_loss : 0.2462, val_loss : 0.2744, train_acc : 0.958, val_acc : 0.958\n",
      "Epoch 61, train_loss : 0.2410, val_loss : 0.2708, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 62, train_loss : 0.2360, val_loss : 0.2674, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 63, train_loss : 0.2311, val_loss : 0.2641, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 64, train_loss : 0.2264, val_loss : 0.2610, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 65, train_loss : 0.2218, val_loss : 0.2579, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 66, train_loss : 0.2173, val_loss : 0.2550, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 67, train_loss : 0.2131, val_loss : 0.2523, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 68, train_loss : 0.2089, val_loss : 0.2496, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 69, train_loss : 0.2049, val_loss : 0.2470, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 70, train_loss : 0.2010, val_loss : 0.2446, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 71, train_loss : 0.1972, val_loss : 0.2422, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 72, train_loss : 0.1935, val_loss : 0.2401, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 73, train_loss : 0.1900, val_loss : 0.2379, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 74, train_loss : 0.1866, val_loss : 0.2358, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 75, train_loss : 0.1833, val_loss : 0.2339, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 76, train_loss : 0.1801, val_loss : 0.2320, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 77, train_loss : 0.1770, val_loss : 0.2303, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 78, train_loss : 0.1740, val_loss : 0.2285, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 79, train_loss : 0.1711, val_loss : 0.2270, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 80, train_loss : 0.1683, val_loss : 0.2254, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 81, train_loss : 0.1656, val_loss : 0.2239, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 82, train_loss : 0.1630, val_loss : 0.2225, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 83, train_loss : 0.1605, val_loss : 0.2212, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 84, train_loss : 0.1580, val_loss : 0.2199, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 85, train_loss : 0.1556, val_loss : 0.2187, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 86, train_loss : 0.1533, val_loss : 0.2175, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 87, train_loss : 0.1511, val_loss : 0.2165, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 88, train_loss : 0.1489, val_loss : 0.2154, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 89, train_loss : 0.1468, val_loss : 0.2144, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 90, train_loss : 0.1448, val_loss : 0.2135, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 91, train_loss : 0.1428, val_loss : 0.2126, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 92, train_loss : 0.1409, val_loss : 0.2117, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 93, train_loss : 0.1390, val_loss : 0.2109, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 94, train_loss : 0.1372, val_loss : 0.2101, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 95, train_loss : 0.1355, val_loss : 0.2094, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 96, train_loss : 0.1338, val_loss : 0.2087, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 97, train_loss : 0.1342, val_loss : 0.2102, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 98, train_loss : 0.1385, val_loss : 0.2278, train_acc : 0.958, val_acc : 0.875\n",
      "Epoch 99, train_loss : 0.1246, val_loss : 0.1841, train_acc : 0.979, val_acc : 0.917\n",
      "test_acc : 1.000\n"
     ]
    }
   ],
   "source": [
    "# 各種変数定義\n",
    "learning_rate = 0.01\n",
    "batch_size = 10\n",
    "num_epochs = 100\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = XXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
    "\n",
    "# 空配列定義\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# ミニバッチイテレータ生成\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train_one_hot, batch_size=batch_size)\n",
    "\n",
    "def example_net(x):\n",
    "    \"\"\"tensorflowを利用したニューラルネットワーク\n",
    "    Parameters\n",
    "    ---------------\n",
    "    x : 入力配列\n",
    "    \"\"\"\n",
    "    # 重み定義\n",
    "    weights = {\n",
    "        'w1': XXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
    "        'w2': XXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
    "        'w3': XXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
    "    }\n",
    "    # バイアス定義\n",
    "    biases = {\n",
    "        'b1': XXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
    "        'b2': XXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
    "        'b3': XXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
    "    }\n",
    "    # 計算グラフ構築（順伝播処理）\n",
    "    layer_1 = XXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
    "    layer_1 = XXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
    "    layer_2 = XXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
    "    layer_2 = XXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
    "    layer_output = XXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
    "    return layer_output\n",
    "\n",
    "# 計算グラフ受け取る\n",
    "logits = example_net(X)\n",
    "# 損失定義\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits)) # 2値分類からの変更箇所\n",
    "# 最適化手法の定義\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "# 最適化手法で、定義した損失を最小化するルールを作成\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "# ACC計算\n",
    "correct_pred = XXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# 変数を扱うためのおまじない\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# tensorflowのセッション開始\n",
    "with tf.Session() as sess:\n",
    "    # 初期化\n",
    "    sess.run(init)\n",
    "    # 学習回数分ループ\n",
    "    for epoch in range(num_epochs):\n",
    "        # ミニバッチイテレータでループ\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            # 「最適化手法で、定義した損失を最小化するルール」を実行\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "        # 記録\n",
    "        train_loss, train_acc = sess.run([loss_op, accuracy], feed_dict={X: X_train, Y: y_train_one_hot})\n",
    "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val_one_hot})\n",
    "        # 仮定出力\n",
    "        print(\"Epoch {}, train_loss : {:.4f}, val_loss : {:.4f}, train_acc : {:.3f}, val_acc : {:.3f}\".format(epoch, train_loss, val_loss, train_acc, val_acc))\n",
    "    \n",
    "    # 学習が終了したらテストデータで実行\n",
    "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test_one_hot})\n",
    "    #prediction = sess.run(logits, feed_dict={X: X_test, Y: y_test})\n",
    "    #print(prediction)\n",
    "    print(\"test_acc : {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 問題4　House Pricesのモデルを作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データ準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path =\"train.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "y = df[\"SalePrice\"]\n",
    "X = df.loc[:, [\"GrLivArea\", \"YearBuilt\"]]\n",
    "y = np.array(y)\n",
    "X = np.array(X)\n",
    "y = y.astype(np.int)[:, np.newaxis]\n",
    "y = np.log(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensorflowで学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss : 3405794.7500, val_loss : 3497204.7500\n",
      "Epoch 1, loss : 1279180.8750, val_loss : 1488025.2500\n",
      "Epoch 2, loss : 509490.4375, val_loss : 501072.9375\n",
      "Epoch 3, loss : 332158.7812, val_loss : 272572.3125\n",
      "Epoch 4, loss : 261774.3281, val_loss : 189810.6094\n",
      "Epoch 5, loss : 221339.1250, val_loss : 149105.0469\n",
      "Epoch 6, loss : 187406.9375, val_loss : 118506.2109\n",
      "Epoch 7, loss : 160066.2344, val_loss : 97116.6406\n",
      "Epoch 8, loss : 136667.9688, val_loss : 83396.4766\n",
      "Epoch 9, loss : 117757.5469, val_loss : 71347.3438\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_mse : 836283.812\n"
     ]
    }
   ],
   "source": [
    "# 各種変数定義\n",
    "learning_rate = 0.001\n",
    "batch_size = 10\n",
    "num_epochs = 10\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 1\n",
    "\n",
    "# 空配列定義\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# ミニバッチイテレータ生成\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
    "\n",
    "def example_net(x):\n",
    "    \"\"\"tensorflowを利用したニューラルネットワーク\n",
    "    Parameters\n",
    "    ---------------\n",
    "    x : 入力配列\n",
    "    \"\"\"\n",
    "    # 重み定義\n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    # バイアス定義\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "    # 計算グラフ構築（順伝播処理）\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3']\n",
    "    return layer_output\n",
    "\n",
    "# 計算グラフ受け取る\n",
    "logits = example_net(X)\n",
    "# 損失定義\n",
    "loss_op =  tf.losses.mean_squared_error(labels=Y, predictions=logits)\n",
    "# 最適化手法の定義\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "# 最適化手法で、定義した損失を最小化するルールを作成\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# 変数を扱うためのおまじない\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# tensorflowのセッション開始\n",
    "with tf.Session() as sess:\n",
    "    # 初期化\n",
    "    sess.run(init)\n",
    "    # 損失記録用リスト\n",
    "    loss_list = []\n",
    "    val_loss_list = []\n",
    "    # 学習回数分ループ\n",
    "    for epoch in range(num_epochs):\n",
    "        # ミニバッチイテレータでループ\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            # 「最適化手法で、定義した損失を最小化するルール」を実行\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "        # 損失計算と格納\n",
    "        loss = sess.run(loss_op, feed_dict={X: X_train, Y: y_train})\n",
    "        val_loss = sess.run(loss_op, feed_dict={X: X_val, Y: y_val})\n",
    "        loss_list.append(loss)\n",
    "        val_loss_list.append(val_loss)    \n",
    "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}\".format(epoch, loss, val_loss))\n",
    "    \n",
    "    # 学習過程可視化\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.plot(loss_list, label='loss')\n",
    "    plt.plot(val_loss_list, label='val_loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # テストデータに適用\n",
    "    test_loss = sess.run(loss_op, feed_dict={X: X_test, Y: y_test})\n",
    "    print(\"test_mse : {:.3f}\".format(test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 問題5　MNISTのモデルを作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データ準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 読み込み\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "#　平滑化\n",
    "X_train = X_train.reshape(-1, 784)\n",
    "X_test = X_test.reshape(-1, 784)\n",
    "\n",
    "# 正規化\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# 変形\n",
    "y_train = y_train.astype(np.int)[:, np.newaxis]\n",
    "y_test = y_test.astype(np.int)[:, np.newaxis]\n",
    "\n",
    "# one-hotベクトル化\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train[:])\n",
    "y_test_one_hot = enc.fit_transform(y_test[:])\n",
    "\n",
    "# 分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train_one_hot, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensorflowで学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train_loss : 39.8951, val_loss : 39.0598, train_acc : 0.540, val_acc : 0.540\n",
      "Epoch 1, train_loss : 25.1115, val_loss : 24.6767, train_acc : 0.625, val_acc : 0.629\n",
      "Epoch 2, train_loss : 25.0157, val_loss : 24.8408, train_acc : 0.633, val_acc : 0.630\n",
      "Epoch 3, train_loss : 21.0147, val_loss : 20.5066, train_acc : 0.669, val_acc : 0.670\n",
      "Epoch 4, train_loss : 19.5649, val_loss : 19.1285, train_acc : 0.687, val_acc : 0.692\n",
      "Epoch 5, train_loss : 20.7867, val_loss : 20.5265, train_acc : 0.694, val_acc : 0.696\n",
      "Epoch 6, train_loss : 19.9716, val_loss : 19.9874, train_acc : 0.713, val_acc : 0.712\n",
      "Epoch 7, train_loss : 18.4316, val_loss : 18.2772, train_acc : 0.732, val_acc : 0.730\n",
      "Epoch 8, train_loss : 19.6212, val_loss : 19.2821, train_acc : 0.740, val_acc : 0.744\n",
      "Epoch 9, train_loss : 18.4677, val_loss : 18.1699, train_acc : 0.770, val_acc : 0.768\n",
      "Epoch 10, train_loss : 21.5396, val_loss : 21.3858, train_acc : 0.747, val_acc : 0.746\n",
      "Epoch 11, train_loss : 18.2798, val_loss : 18.4660, train_acc : 0.776, val_acc : 0.772\n",
      "Epoch 12, train_loss : 21.0845, val_loss : 21.3816, train_acc : 0.759, val_acc : 0.755\n",
      "Epoch 13, train_loss : 19.2766, val_loss : 19.4463, train_acc : 0.770, val_acc : 0.765\n",
      "Epoch 14, train_loss : 23.2208, val_loss : 23.4168, train_acc : 0.768, val_acc : 0.762\n",
      "Epoch 15, train_loss : 19.3213, val_loss : 19.3945, train_acc : 0.797, val_acc : 0.793\n",
      "Epoch 16, train_loss : 20.5458, val_loss : 20.5295, train_acc : 0.789, val_acc : 0.785\n",
      "Epoch 17, train_loss : 19.6442, val_loss : 19.9040, train_acc : 0.809, val_acc : 0.805\n",
      "Epoch 18, train_loss : 20.2844, val_loss : 20.7265, train_acc : 0.791, val_acc : 0.787\n",
      "Epoch 19, train_loss : 22.6262, val_loss : 22.5209, train_acc : 0.787, val_acc : 0.782\n",
      "test_acc : 0.791\n"
     ]
    }
   ],
   "source": [
    "# 各種変数定義\n",
    "learning_rate = 0.003\n",
    "batch_size = 1\n",
    "num_epochs = 20\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 10 # 2値分類からの変更箇所\n",
    "\n",
    "# 空配列定義\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# ミニバッチイテレータ生成\n",
    "get_mini_batch_train = GetMiniBatch(X_train[:1000], y_train[:1000], batch_size=batch_size)\n",
    "\n",
    "def example_net(x):\n",
    "    \"\"\"tensorflowを利用したニューラルネットワーク\n",
    "    Parameters\n",
    "    ---------------\n",
    "    x : 入力配列\n",
    "    \"\"\"\n",
    "    # 重み定義\n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    # バイアス定義\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "    # 計算グラフ構築（順伝播処理）\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3']\n",
    "    return layer_output\n",
    "\n",
    "# 計算グラフ受け取る\n",
    "logits = example_net(X)\n",
    "# 損失定義\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits)) # 2値分類からの変更箇所\n",
    "# 最適化手法の定義\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "# 最適化手法で、定義した損失を最小化するルールを作成\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "# ACC計算\n",
    "correct_pred = tf.equal(tf.argmax(Y, 1), tf.argmax(logits, 1)) # 2値分類からの変更箇所\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# 変数を扱うためのおまじない\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# tensorflowのセッション開始\n",
    "with tf.Session() as sess:\n",
    "    # 初期化\n",
    "    sess.run(init)\n",
    "    # 学習回数分ループ\n",
    "    for epoch in range(num_epochs):\n",
    "        # ミニバッチイテレータでループ\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            # 「最適化手法で、定義した損失を最小化するルール」を実行\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "        # 記録\n",
    "        train_loss, train_acc = sess.run([loss_op, accuracy], feed_dict={X: X_train, Y: y_train})\n",
    "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
    "        # 仮定出力\n",
    "        print(\"Epoch {}, train_loss : {:.4f}, val_loss : {:.4f}, train_acc : {:.3f}, val_acc : {:.3f}\".format(epoch, train_loss, val_loss, train_acc, val_acc))\n",
    "    \n",
    "    # 学習が終了したらテストデータで実行\n",
    "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test_one_hot})\n",
    "    #prediction = sess.run(logits, feed_dict={X: X_test, Y: y_test})\n",
    "    #print(prediction)\n",
    "    print(\"test_acc : {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
